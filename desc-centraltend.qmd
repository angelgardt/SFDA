# Меры центральной тенденции {#desc-centraltend}

```{r opts, echo=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, error = FALSE)
```

```{r pkgs}
library(tidyverse)
theme_set(theme_bw())
```

Мы знаем, что наши переменные-признаки могут быть некоторым образом распределены --- как в генеральной совокупности, так и в выборке. Как именно они распределены, описывается распределением случайной величины.

Вот мы собрали некоторые данные и получили какое-то эмпирическое распределение наших переменных. Нам бы, конечно, хотелось понять, что там такое за распределение. И первым шагом к пониманию этого будет *описание* распределения.


## Виды статистики {#centraltend-stats}

Вообще статистика [как набор методов и инструментов] делится на два вида:

- **Описательная статистика (descriptive statistics[^desc-stats1])** занимается обработкой статистических данных, их наглядным представлением, и собственно описанием через некоторые характеристики.
    - Эти характеристики, количественно описывающие особенности имеющихся данных, называются **описательными статистиками (descriptive statistics[^desc-stats2]).**
    - Задача описательной статистики --- ёмко описать имеющиеся данные и составить на основе этих описаний общее представление о них, а также обнаружить особенности, которые могут повлиять на дальнейший анализ.
- **Статистика вывода (inferential statistics)** занимается поиском ответов на содержательные вопросы, которые мы задаем данным в ходе их анализа в рамках научных и практических исследований.
    - Состоит из двух компонентов --- **тестирования статистических гипотез** и **статистических методов**.

[^desc-stats1]: Mass (uncountable) noun.
[^desc-stats2]: Countable noun, plural in this case.

:::{.callout-note}
### Замечание о машинном обучении

Вы наверняка не раз слышали словосочетание «машинное обучение». Это что-то, что время от времени становится то менее, то более хайпово. На самом деле, статистические методы лежат где-то между статистикой вывода и машинным обучением.

Почему?

Дело в том, что на статистические методы можно смотреть по-разному.

- Если нашей задачей является *поиск ответов на исследовательские вопросы* о закономерностях, о связи каких-либо факторов или влиянии переменных друг на друга, то мы будем смотреть на статистические модели с точки зрения *статистики вывода*. Это позволит нам находить ответы на интересующие нас вопросы --- причем не важно, говорим мы о научных исследованиях или об исследованиях в индустрии.
- Если перед нами стоит задача хорошо *предсказывать одни переменные на основании значений других* --- например, выдавать рекомендации на Яндекс Музыке или в Яндекс Лавке --- то мы будем смотреть на те же статистические модели с точки зрения *машинного обучения*.

То есть, модели абсолютно одни и те же, но то, какую модель мы назовем хорошей и как мы эту «хорошесть» определим, будет различаться в зависимости от задачи --- исследовательская или предиктивная --- которая перед нами стоит.
:::

Мы начнем знакомиться со статистикой с описательной статистики, а именно с мер центральной тенденции.



## Меры центральной тенденции {#centraltend-centraltend}

Итак, мы хотим описать наши данные. Точнее, распределения переменных, которые у нас в данных есть. Хотим мы сделать это просто и ёмко. Насколько просто и ёмко? Ну, допустим максимально --- одним числом. Кажется, значение переменной, которое лежит в центре распределения, неплохо для этого подойдет.

Как мы будем искать, что там в центре распределения? Зависит от шкалы, в которой измерена конкретная переменная.

| Шкала | Мера центральной тенденции |
|:---|:---|
| Номинальная |	Мода |
| Порядковая | Медиана |
| Интервальная | Среднее арифметическое |
| Абсолютная | Среднее арифметическое, геометрическое и др. |

Однако есть некоторые нюансы.


### Мода {#centraltend-mode}

:::{#def-mode}
**Мода (mode)** --- наиболее часто встречающееся значение данной переменной.
:::

Тут все достаточно просто и интуитивно понятно. Пусть у нас есть следующий ряд наблюдений:

```{r}
x <- c(1, 3, 4, 6, 4, 2, 4, 3, 2, 4, 1)
x
```

Если мы составим *таблицу частот*, то получим следующее:

```{r}
table(x)
```

Очевидно, что $4$ встречается чаще других значений --- это и есть мода.

Понятно, что если на нашей шкале нет чисел, а есть текстовые лейблы, это ничего не меняет:

```{r}
y <- c("Москва", "Казань", "Кёнигсберг", "Барнаул (Алтайский край)", "Москва", "Санкт-Петербург", "Санкт-Петербург", "Москва", "Санкт-Петербург", "Москва", "Кёнигсберг", "Санкт-Петербург", "Москва", "Казань", "Санкт-Петербург", "Санкт-Петербург", "Казань", "Казань", "Санкт-Петербург", "Москва", "Москва", "Санкт-Петербург", "Санкт-Петербург", "Санкт-Петербург", "Санкт-Петербург", "Москва", "Кёнигсберг", "Санкт-Петербург", "Казань")
y
```

```{r}
table(y)
```

Мода, получается, Санкт-Петербург.

Так мы поступаем с эмпирическим распределением. Если мы имеем дело с генеральной совокупностью, то можем формально определить моду через функцию вероятности (probability mass function, PMF). Модой будет являться значение случайной величины, при котором PMF принимает своё максимальное значение.

$$
\text{mode}(X) = \arg \max \big( \text{PMF}(X) \big)
$$

:::{#fig-mode-discrete}
```{r}
tibble(x = 1:10,
       y = c(.01, .03, .07, .1, .1, .15, .2, .1, .09, .15)) |> 
  ggplot(aes(x, y)) +
  annotate(geom = "point", x = 7, y = 0.2, size = 7, shape = 21, color = "darkred", fill = "red", alpha = .5) +
  geom_point() +
  geom_vline(xintercept = 7, color = "darkred", linetype = "dashed") +
  annotate(geom = "text", label = "это максимум функции", 
           x = 8.5, y = 0.2, color = "darkred") +
  annotate(geom = "text", label = "это мода", 
           x = 7, y = 0, color = "darkred") +
  scale_x_continuous(breaks = 1:10) +
  labs(x = "Значение", y = "Вероятность")
```

Определение моды для дискретной случайной величины по функции вероятности
:::

Мода --- 7.



### Медиана {#centraltend-median}

:::{#def-median}
Медиана (median) --- значение переменной, которые располагается на середине отсортированного ряда значений.
:::

То есть, она делит все наблюдения переменной ровно пополам, и половина наблюдений оказывается по одну сторону от медианы, а половина --- по другую.

Если у нас нечетное число наблюдений, то всё ясно. Пусть есть такой ряд наблюдений:

```{r}
v1 <- c(2, 3, 14, 9, 16, 19, 28, 7, 26, 18, 1)
v1
```

Отсортируем его:

```{r}
sort(v1)
```

Посмотрим в середину --- найдем медиану:

```{r}
median(v1)
```

А что делать, если число наблюдений чётное? Ведь тогда середина ряда будет между двух чисел. Ну, возьмем их среднее арифметическое --- это и будет медиана.

Возьмём такой ряд наблюдений:

```{r}
v2 <- c(14, 10, 9, 16, 30, 3, 25, 8, 18, 7)
v2
```

Отсортируем:

```{r}
sort(v2)
```

Найдём медиану как $(10 + 14) / 2$:

```{r}
median(v2)
```

Формально это можно написать так:

$$
\text{median}(X) = \cases{
X_{\frac{n+1}{2}}, \quad \text{if} \, n \, \text{is odd} \\
\frac{X_{\frac{n}{2}} + X_{\frac{n}{2}+1}}{2}, \quad \text{otherwise}
}
$$

где $X$ --- ряд наблюдений данной переменной, $n$ --- число наблюдений, $X_i$ --- наблюдение с индексом $i$ в сортированном векторе $X$.



### Среднее арифметическое {#centraltend-mean}

С этим существом все знакомы еще со школы.

:::{#def-mean}
**Арифметическое среднее (arithmetic mean, mean, average)** --- сумма всех значений переменной, делёная на их количество.
:::

Иначе говоря:

$$
M(X) = \overline{X} = \frac{1}{n} \sum_{i=1}^n x_i
$$

где $\overline X$ --- среднее арифметическое, $x_i$ --- наблюдение в векторе $X$, $n$ --- количество наблюдений.

Ну, то есть всё сложить и поделить на количество того, чего сложили. Изи.

Вот, скажем, средние по двум рядам наблюдений, которые мы встречали в разделе про медиану:

```{r}
v1
mean(v1)
```

```{r}
v2
mean(v2)
```


### Среднее взвешенное {#centraltend-weighted-mean}

Часто возникает такая ситуация, когда нам нужно посчитать среднее по каким-либо имеющимся характеристикам, но одни характеристики для нас важнее, чем другие. Например, мы хотим вычислить суммарный балл обучающегося за курс на основе ряда работ, выполненных в течение курса. Мы могли бы взять оценки за все работы и усреднить их. Однако мы понимаем, что, скажем, тест из десяти вопросов с множественным выбором явно менее показателен, чем, например, аналитическое эссе или экзаменационная оценка. Что делать? Взвесить параметры!

Что значит *взвесить*? Умножить на некоторое число. На самом деле, любое. Пусть мы посчитали, что написать эссе в три абстрактных раза тяжелее, чем написать тест, а сдать экзамен в два раза тяжелее, чем написать эссе. Тогда мы можем присвоить баллу за тест вес $1$, баллу за аналитическое эссе вес $3$, а экзамену --- вес $6$. Тогда итоговая оценка за курс будет рассчитываться следующим образом:

$$
\text{final score} = 1 \cdot \text{test} + 3 \cdot \text{essay} + 6 \cdot \text{exam}
$$

Суперкласс. Однако! Весьма вероятно, что в учебном заведении принята единая система оценивания для всех видов работ --- ну, скажем, некая абстрактная десятибалльная система в сферическом вакууме. Получается, если и за тест, и за эссе, и за экзамен у студента по 10 баллов, то суммарный балл 100, что, кажется, больше, чем 10. Чтобы вернуться к изначальным границам баллов, нужно поделить суммарный балл на сумму весов параметров:

$$
\text{final score} = 
\frac{1 \cdot \text{test} + 3 \cdot \text{essay} + 6 \cdot \text{exam}}
{1 + 3 + 6}
$$

Кайф! Собственно, это и есть взвешенное среднее. Коэффициенты, на которые мы умножаем значения переменных, называются *весами*. В общем виде формула принимает следующий вид:

$$
\overline X = \frac{\sum_{i=1}^n w_i \cdot x_i}{\sum_{i=1}^n w_i},
$$

где $x_i$ --- значения переменной, $w_i$ --- веса для этих значений.

Взвешенное среднее часто применяется именно во всякого рода ассессментах, и не только образовательных. Например, вы HR-аналитик и оцениваете персонал. Вы аналитически вычисляете веса коэффициентов (допустим, с помощью линейной регрессии), а далее на их основе высчитываете интегральный балл, по которому будете оценивать сотрудников. Это как один из индустриальных примеров.



### Другие средние {#centraltend-other}

Среднее бывает не только арифметическое. Правда встретятся вам другие его виды примерно нигде --- то есть о-о-о-очень редко и, скорее всего, в каких-то узкоспециализированных статьях. Но упомянуть их, пожалуй, стоит как минимум ради того, чтобы вы не перепугались излишне, ежели вдруг с ними столкнётесь.


#### Квадратичное среднее {#centraltend-rms}

Это весьма полезная вещь.

:::{#def-quadmean}
**Квадратичное среднее (quadratic mean, root mean square, RMS)** --- это квадратный корень из среднего квадрата наблюдений.
:::

Ничего не понятно, поэтому по порядку:

- есть наблюдение $x_i$
- значит есть и его квадрат $x_i^2$
- мы умеем считать обычно среднее арифметическое
    - но ведь $x_i^2$ --- это тоже наблюдение (число), просто в квадрате
- значит можем посчитать среднее арифметическое квадратов наблюдений --- средний квадрат (mean square) --- $\displaystyle \frac{1}{n} \sum_{i=1}^n x_i^2$
- теперь извлечём из этого дела корень, чтобы вернуться к исходным единицам измерения --- получим то, что нам надо

$$
\text{RMS}(X) = \sqrt{\frac{1}{n} \sum_{i=1}^n x_i^2}
$$

Per se[^per-se] мы его вряд ли ещё когда-то увидим, но пару раз оно внезапно всплывет.

[^per-se]: Per se (*лат.*) --- в чистом виде.



#### Геометрическое среднее {#centraltend-geom}

В психологии и социальных науках встречается редко. Применяется там, где необходимо изучать средние скорости изменения --- например, в экономике и финансах при изучении доходности, прибыли и выручки; в демографии при расчете индекса человеческого потенциала и др.

$$
G(X) = \sqrt[n]{\prod_{i=1}^n x_i}
$$



#### Гармоническое {#centraltend-harm}

Весьма экзотическая конструкция, используемая при работе с величинами, заданными через обратные значения. Встречается в финансах и экономике, страховании, физике.

$$
H(X) = \frac{n}{\displaystyle \sum_{i=1}^n \frac{1}{x_i}}
$$




## Сравнение мер центральной тенденции {#centraltend-comparison}

Сравнивать будем моду, медиану и среднее [арифметическое].

Все три статистики --- мода, медиана и среднее --- описывают **центральную тенденцию** --- значение изучаемой нами переменной, вокруг которого собираются другие значения. Но если их три и все они используются, значит между ними должны быть какие-то различия. Посмотрим, какие.


### Меры центральной тенденции и типы переменных {#centraltend-comparison-vars}

- Во-первых, очевидно, что **моду невозможно посчитать для непрерывной переменной**.

:::{.callout-note appearance="minimal" collapse="true"}
#### Нет, не очевидно

Так как вероятность того, что непрерывная случайная величина принимает своё конкретное значение, равна нулю, каждое наблюдение в нашей выборке будет уникально --- встретится ровно один раз. Вспомните пример из предыдущей главы (@fig-unif-10000), где мы набирали числа из отрезка. Получается, что мода теряет свой смысл.
:::

- Во-вторых, **медиану нельзя посчитать на номинальной шкале**. Кстати, почему?

:::{.callout-note appearance="minimal" collapse="true"}
#### Потому что

на номинальной шкале нет отношения порядка между элементами --- на ней нельзя сравнивать на больше-меньше. Следовательно, невозможно отсортировать наблюдения, а значит, и найти медиану.
:::

- В-третьих, **среднее тоже нельзя посчитать на номинальной шкале**.

:::{.callout-note appearance="minimal" collapse="true"}
#### Можно, но осторожно

Вообще, конечно, да --- нельзя, потому что на номинальной шкале не определена операция сложения, входящая в вычисление среднего. Однако если на номинальной шкале есть только две категории, которые закодированы `0` и `1`, то посчитать среднее можно. Но что оно будет значить?

Исходный математический смысл среднего явно утерян. Посмотрим на это по-другому: посчитать сумму единиц это всё равно, что посчитать количество единиц. То есть, если мы сложим все нули и единицы, то получим количество единиц среди всех наших наблюдений. А разделив количество единиц на количество наблюдений, мы получим долю единиц --- то есть долю наблюдений с лейблом `1`.

окак
:::

- В-четвертых, **для дискретной переменной значение среднего арифметического будет не особо осмысленно**. Ну, скажем, странно сказать, что в аудитории в среднем стоят 15.86 столов или в российских семьях в среднем 1.5 ребенка. Конечно, в ряде случаев можно это как-то более-менее содержательно интерпретировать, но это требует усилий, а мы ленивые, поэтому лучше использовать медиану.


:::{.callout-warning}
#### Итого, делаем следующие выводы

- для номинальной шкалы пригодна только мода
- для дискретных переменных подходят мода и медиана
- мода иногда лучше, так как точно всегда будет целым числом
- для непрерывных переменных подходят медиана и среднее
:::



### Меры центральной тенденции и форма распределения {#centraltend-comparison-shape}

Помимо того, что среднее, мода и медиана информативны сами по себе, полезно смотреть на их взаимное расположение.

```{r central_tendency_sampling, include=FALSE}
set.seed(108)
symm <- sample(
  x = seq(1, 10, 0.5),
  size = 600,
  replace = TRUE,
  prob = c(
    .05,
    .05,
    .07,
    .1,
    .1,
    .15,
    .20,
    .30,
    .35,
    .5,
    .35,
    .30,
    .20,
    .15,
    .1,
    .1,
    .07,
    .05,
    .05
  )
)
asymm_right <- sample(
  x = seq(1, 10, 0.5),
  size = 600,
  replace = TRUE,
  prob = c(
    .1,
    .2,
    .25,
    .4,
    .5,
    .5,
    .4,
    .35,
    .3,
    .25,
    .2,
    .25,
    .2,
    .15,
    .1,
    .1,
    .07,
    .05,
    .05
  )
)
asymm_left <- sample(
  x = seq(1, 10, 0.5),
  size = 600,
  replace = TRUE,
  prob = c(
    .03,
    .05,
    .07,
    .1,
    .15,
    .15,
    .2,
    .2,
    .25,
    .25,
    .3,
    .35,
    .5,
    .5,
    .4,
    .4,
    .25,
    .2,
    .2
  )
)
bimodal <- sample(
  x = seq(1, 10, 0.5),
  size = 600,
  replace = TRUE,
  prob = c(
    .05,
    .05,
    .07,
    .1,
    .1,
    .2,
    .3,
    .35,
    .3,
    .15,
    .1,
    .15,
    .20,
    .40,
    .50,
    .25,
    .1,
    .05,
    .05
  )
)
colors <- c("Mean" = "red4", "Median" = "blue4", "Mode" = "green4")
```

- На **симметричном распределении** мода, медиана и среднее **совпадают** [или, по крайней мере, находятся очень близко друг к другу].

> Здесь и далее: красная (сплошная) линия --- среднее, синяя (короткопунктирная) --- медиана, зелёная (длиннопунктирная) --- мода.

:::{#fig-centraltend-symm}
```{r central_tendency_symm, echo=FALSE}
ggplot(NULL, aes(symm)) +
  geom_histogram(aes(y = ..density..), alpha =.5, binwidth = .5) +
  geom_density() +
  geom_vline(xintercept = mean(symm),
             color = colors['Mean'],
             linewidth = 1) +
  geom_vline(xintercept = median(symm), 
             color = colors['Median'],
             linewidth = 1,
             linetype = "dashed") +
  geom_vline(xintercept = mean(symm)+.04,
             color = colors['Mode'],
             linewidth = 1,
             linetype = "longdash") +
  labs(x = 'Value',
       y = 'Density')
```

Взаимное расположение моды, медианы и среднего на симметричном распределении
:::


- На **асимметричном** распределении **мода** [практически] **в пи́ке**

.Практически, потому что мода для непрерывной переменной определена по функции плотности вероятности [черная линия на графике], которая не всегда точно аппроксимирует (в данном случае то же, что и сглаживает) эмпирическое распределение. На картинке ниже мы видим, что по гистограмме мода должна была бы быть в районе самого высокго столбика, однако при сглаживании гистограммы пик немного съехал, и мода оказалась в вершине графика функции плотности вероятности.

:::{#fig-centraltend-asymm-right}
```{r central_tendency_asymm_right, echo=FALSE}
ggplot(NULL, aes(asymm_right)) +
  geom_histogram(aes(y = ..density..), alpha =.5, binwidth = .5) +
  geom_density() +
  geom_vline(xintercept = mean(asymm_right),
             linewidth = 1,
             color = colors['Mean']) +
  geom_vline(xintercept = median(asymm_right),
             linewidth = 1,
             linetype = "dashed",
             color = colors['Median']) +
  geom_vline(xintercept = 3,
             linewidth = 1,
             linetype = "longdash",
             color = colors['Mode']) +
  labs(x = 'Value',
       y = 'Density')
```
Взаимное расположение моды, медианы и среднего на распределении с правосторонней асимметрией
:::

:::{#fig-centraltend-asymm-left}
```{r central_tendency_asymm_left, echo=FALSE}
ggplot(NULL, aes(asymm_left)) +
  geom_histogram(aes(y = ..density..), alpha =.5, binwidth = .5) +
  geom_density() +
  geom_vline(xintercept = mean(asymm_left),
             linewidth = 1,
             color = colors['Mean']) +
  geom_vline(xintercept = median(asymm_left),
             linewidth = 1,
             linetype = "dashed",
             color = colors['Median']) +
  geom_vline(xintercept = 7.4,
             linewidth = 1,
             linetype = "longdash",
             color = colors['Mode']) +
  labs(x = 'Value',
       y = 'Density')
```

Взаимное расположение моды, медианы и среднего на распределении с левосторонней асимметрией
:::

- На **асимметричном** распределении **медиана и среднее** смещены в сторону хвоста. **Среднее смещено сильнее медианы**.

Это связано с тем, что медиана зависит только от количества наблюдений, а среднее --- ещё и от самих значений. На картинках выше примеры для распределения *с правосторонней асимметрией* (потому что хвост справа) --- среднее (красная линия) правее медианы (синяя линия) --- и *левосторонней асимметрией* (так как хвост слева) --- среднее (красная линия) левее медианы (синяя линия).

Для того, чтобы лучше разобраться с тем, как экстремальные значения влияют на моду и медиану посмотрим такой пример. Пусть у нас есть оценки за выпускную квалификационную работу. Например, такие:

```{r}
marks <- c(6, 7, 7, 8, 8)
marks
```

Среднее --- `r mean(marks)`, медиана --- `r median(marks)`. Если округлить среднее, то можно считать, что среднее и медиана совпали. Ну, ок.

Но в комиссии сидят ещё два требовательных доктора наук, которые поставили оценки, сильно отличающиеся от остальных:

```{r}
marks2 <- c(6, 7, 7, 8, 8, 3, 4)
marks2
```

Посчитаем медиану и среднее теперь. Среднее --- `r mean(marks2)`, медиана --- `r median(marks2)`.
Медиана осталась на месте, а вот среднее теперь округлится до `r round(mean(marks2))`. Казалось бы, это немного, но в смысле оценок --- это прилично, и может сильно повлиять на GPA.

Итого, **среднее более чувствительно к нетипичным значениям** (очень большим или очень малым).

Есть ещё один интересный вариант распределений --- **бимодальные**. Посмотрим пример ниже:

:::{#fig-centraltend-bimod}
```{r central_tendency_bimodal, echo=FALSE}
ggplot(NULL, aes(bimodal)) +
  geom_histogram(aes(y = ..density..), alpha =.5, binwidth = .5) +
  geom_density() +
  geom_vline(xintercept = mean(bimodal),
             linewidth = 1,
             color = 'red4') +
  geom_vline(xintercept = median(bimodal),
             linewidth = 1,
             linetype = "dashed",
             color = 'blue4') +
  geom_vline(xintercept = 7.8,
             linewidth = 1,
             linetype = "longdash",
             color = 'green4') +
  geom_vline(xintercept = 4.4,
             linewidth = 1,
             linetype = "dotted",
             color = 'green4') +
  labs(x = 'Value',
       y = 'Density')
```

Взаимное расположение моды, медианы и среднего на бимодальном распределении
:::


Мы видим, что на графике есть два пика, однако строго математически мода всё же одна (зелёная пунктирная линия) --- и она в более высоком пике. Это логично, ибо там самые часто встречающиеся (самые вероятные) значения.

И все жё содержательно мы не можем пренебречь вторым пиком (зелёная точечная линия). Почему он нам важен? Обычно бимодальное распределение --- это повод задуматься о том, что наша выборка неоднородна. Бимодальное распределение как бы сложено из двух с центрами в двух пиках. Иначе говоря, в нашей выборке как будто бы две подвыборки, которые обладают разными распределениями интересующего нам признака.

Что с этим делать? Хорошо всегда иметь в данных какие-либо дополнительные переменные --- как минимум соцдем --- чтобы мы могли по данным попытаться предположить, какую группировку мы могли забыть учесть при планировании исследования.

Со средним и медианой в случае бимодального распределения происходит примерно то же, что и в случае асимметричного распределения --- второй пик смещает  к себе обе меры центральной тенденции, причем среднее вновь сильнее, чем медиану.


## Свойства среднего арифметического {#centraltend-mean-features}

Мы упомянем три основных свойства, к которым будем обращаться в последующих темах.

:::{#prp-mean1}
Если к каждому значению распределения прибавить некоторое число (константу), то среднее изменится на ту же константу.

$$
M_{X+c} = M_X + c
$$
:::

:::{.proof}
$$
\begin{split}
M_{X+c} &= \frac{\sum_{i=1}^n (x_i + c)}{n} = \\
&= \frac{\sum_{i=1}^n x_i + nc}{n} = \\
&= \frac{\sum_{i=1}^n x_i}{n} + c = M_X + c
\end{split}
$$
:::

Иначе говоря, распределение просто сдвинется. Например, если к каждому значению синего распределения прибавить $2$, получится красное:

```{r creating_tibble_for_feature_vis_1, include=FALSE}
smpl1 <- tibble(x1 = seq(-3, 3, by = .001),
               y1 = dnorm(x1),
               x2 = x1 + 2,
               y2 = dnorm(x2, mean = 2))
```

```{r mean_feature_1, echo=FALSE}
smpl1 %>% 
  ggplot() +
  geom_line(aes(x1, y1), color = "blue4") +
  geom_line(aes(x2, y2), color = "red4") +
  geom_vline(xintercept = 0, linetype = "dashed", color = "blue4") +
  geom_vline(xintercept = 2, linetype = "dashed", color = "red4") +
  labs(x = "Value", y = "Density")
```


:::{#prp-mean2}
Если каждое значение распределения умножить на некоторое число (константу), то среднее изменится во столько же раз.

$$
M_{X \times c} = M_X \times c
$$
:::

:::{.proof}
$$
\begin{split}
M_{X \times c} &= \frac{\sum_{i=1}^n (x_i \times c)}{n} = \\
&= \frac{c \times \sum_{i=1}^n x_i}{n} = \\
&= \frac{\sum_{i=1}^n x_i}{n} \times c = M_X \times c
\end{split}
$$
:::

Например, здесь каждое значение синего распределения умножили на $3$ и получили красное:

```{r creating_tibble_for_feature_vis_2, include=FALSE}
smpl2 <- tibble(x1 = seq(-2, 4, by = .001),
               y1 = dnorm(x1, mean = 1),
               x2 = x1 * 3,
               y2 = dnorm(x2, mean = 3, sd = 3))
```

```{r mean_feature_2, echo=FALSE}
smpl2 %>% 
  ggplot() +
  geom_line(aes(x1, y1), color = "blue4") +
  geom_line(aes(x2, y2), color = "red4") +
  geom_vline(xintercept = 1, linetype = "dashed", color = "blue4") +
  geom_vline(xintercept = 3, linetype = "dashed", color = "red4") +
  labs(x = "Value", y = "Density")
```

Тут, правда, явно что-то ещё произошло, но мы пока этого не знаем.


:::{#prp-mean3}
Сумма отклонений от среднего значения равна нулю.

$$
\sum_{i=1}^n(x_i - M_X) = 0
$$
:::

:::{.proof}

$$
\begin{split}
\sum_{i=1}^n(x_i - M_X) &= \sum_{i=1}^n x_i - \sum_{i=1}^n M_X = \\ 
& = \sum_{i=1}^n x_i - nM_X = \\
& = \sum_{i=1}^n x_i - n \times \frac{1}{n} \sum_{i=1}^n x_i = \\
&= \sum_{i=1}^n x_i - \sum_{i=1}^n x_i = 0
\end{split}
$$
:::


Но можно это осмыслить и более просто графически.

```{r df_polygons, include=FALSE}
poly_left <- smpl1 %>% 
  select(x1, y1) %>% 
  filter(x1 < 0) %>% 
  bind_rows(tibble(x1 = c(0, -3), y1 = c(0, 0)))
poly_right <- smpl1 %>% 
  select(x1, y1) %>% 
  filter(x1 > 0) %>% 
  bind_rows(tibble(x1 = c(3, 0), y1 = c(0, 0)))
```

```{r zero_deviation_sum, echo=FALSE}
smpl1 %>% 
  ggplot() +
  geom_line(aes(x1, y1)) +
  geom_polygon(data = poly_left, aes(x=x1, y=y1), fill="red4", alpha = .5) +
  geom_polygon(data = poly_right, aes(x=x1, y=y1), fill="green4", alpha = .5) +
  geom_vline(xintercept = 0, linetype = "dashed") +
  annotate(geom="text", x = -1, y = .05, label ="отрицательные\nотклонения") +
  annotate(geom="text", x = 1, y = .05, label ="положительные\nотклонения") +
  labs(x = "Value", y = "Density")
```

:::{#def-deviation}
**Отклонение** --- это разность между конкретным значением переменной и средним по этой переменной.

$$
d_i = \overline X - x_i
$$
:::

И, действительно, так как среднее находится в центре распределения, то часть значений лежит справа, а часть слева. Значит, будут как положительные, так и отрицательные отклонения --- и их сумма в итоге будет равна нулю.
