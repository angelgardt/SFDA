# Нормальное распределение

```{r opts, echo=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, error = FALSE)
```

```{r}
library(tidyverse)
theme_set(theme_bw())
theme_update(legend.position = "bottom")
```

Мы научились описывать эмпирическое распределение переменных --- то есть распределение, которое получено на выборке. Но ведь в генеральной совокупности наши переменные тоже каким-то образом распределены. Уделим внимание и этому вопросу.

## Распределение признаков в генеральной совокупности

Можем ли мы знать наверняка, как распределен признак в генеральной совокупности?

Нет.

Однако мы можем предполагать некоторое теоретическое распределение признака. На основе чего мы можем сделать такое предположение? Конечно же, на основе собранных данных, то есть на основании выборки. Большой выборки. Очень большой выборки. Большого количества очень больших выборок.

Теоретических распределений существует много --- и они разные по форме, по параметрам, которыми они задаются, по величинам, которые ими можно описывать и т. д. Однако есть одно распределение, которое стало невероятно популярным и крайне широко используемым --- нормальное распределение. Его мы и будем рассматривать.

## Нормальное распределение

Тот факт, что некоторая величина распределена согласно закону **нормального распределения (normal distribution)**, записывается следующим образом:

$$
X \thicksim \mathcal{N}(\mu, \sigma^2)
$$

Здесь $X$ --- некоторая случайная величина (переменная), $\mathcal{N}$ --- обозначение нормального распределения, $\mu$ и $\sigma^2$ --- параметры нормального распределения.

### Параметры нормального распределения

Итак, в скобках указаны параметры распределения --- как можно видеть, их всего два. На самом деле, мы их уже хорошо знаем:

- $\mu$ --- это не что иное, как среднее,
- $\sigma^2$ --- дисперсия.

Эти два параметра входят в формулу, описывающую график функции плотности вероятности нормального распределения:

$$
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e ^{- \frac{(x-\mu)^2}{2 \sigma^2}}, \quad x \in \mathbb{R}, \mu \in \mathbb{R}, \sigma \in \mathbb{R}_{+}
$$

А сам график выглядит вот так:

:::{#fig-normdist-general}
```{r}
ggplot() +
  geom_function(fun = dnorm) +
  scale_x_continuous(breaks = -4:4,
                     limits = c(-4, 4),
                     labels = c("",
                                "μ-3σ",
                                "μ-2σ",
                                "μ-σ",
                                "μ",
                                "μ+σ",
                                "μ+2σ",
                                "μ+3σ",
                                "")) +
  labs(x = "Значение", y = "Плотность вероятности")
```

Общий вид нормального распределения
:::



И мы его уже тоже много раз видели.


:::{.callout-note}
#### Почему все так любят нормальное распределение?

- Его очень давно знают. Карл Фридрих Гаусс (1777–1855) исчерпывающе его исследовал, и теперь про это распределение известно всё. И ещё чуть более.
- Ряд статистических методов, называемых параметрическими, ~~требуют~~, чтобы распределение изучаемых переменных подчинялось нормальному закону.
    - Сейчас, строго говоря, это уже не совсем так. Появляются новые исследования и симуляции, показывающие, что это далеко не ключевое требование.
- С помощью нормального распределения определяют статистические нормы. Например, в образовательном тестировании, психодиагностике и иногда клинической практике.
- На основании нормального распределения рассчитывается стандартная ошибка среднего --- важная оценка в статистике.
- На основании нормального распределения, а точнее --- стандартной ошибки --- рассчитываются доверительные интервалы среднего --- одна из ключевых оценок в статистике.

:::


### Форма нормального распределения и параметры

Очевидно, что раз у распределения есть какие-то параметры, значит они каким-то образом на него влияют. Не менее очевидно, что среднее $\mu$ будет задавать положение центра колокола на оси $x$, а дисперсия $\sigma^2$ --- ширину колокола. Ниже представлены несколько нормальных распределений c различными параметрами:

:::{#fig-normdists-params}
```{r}
tibble(
  x = seq(-10, 10, .05),
  "N(0, 1)" = dnorm(x, mean = 0, sd = 1),
  "N(−0.5, 0.25)" = dnorm(x, mean = -.5, sd = .5),
  "N(0, 2.25)" = dnorm(x, mean = 0, sd = 1.5),
  "N(1, 1)" = dnorm(x, mean = 1, sd = 1),
  "N(2, 4)" = dnorm(x, mean = 2, sd = 2)
) %>% 
  pivot_longer(cols = -x) %>% 
ggplot(aes(x, value, color = name)) +
  geom_line() +
  labs(x = "Значение", y = "Плотность вероятности",
       color = "Параметры")
```

Вид функции плотности нормального распределения при различных параметрах
:::


### Стандартные отклонения и вероятности

Как уже упоминалось выше, нормальное распределение изучено вдоль и поперек. В том числе, посчитаны вероятности попадания значений в определенные интервалы. Вот они:

:::{#fig-normdist-probs}
```{r}
ggplot() +
  stat_function(fun = dnorm) +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(-1, 1),
                alpha = .5, fill = "royalblue") +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(-2, -1),
                alpha = .5, fill = "springgreen") +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(1, 2),
                alpha = .5, fill = "springgreen") +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(-3, -2),
                alpha = .5, fill = "gold") +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(2, 3),
                alpha = .5, fill = "gold") +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(-4, -3),
                alpha = .5, fill = "salmon") +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(3, 4),
                alpha = .5, fill = "salmon") +
  geom_vline(xintercept = 0, linetype= "dashed") +
  geom_vline(xintercept = c(-4, -3, -2, -1, 1, 2, 3, 4), linetype= "dotted") +
  annotate(geom = "text", label = "0.1%", x = -3.5, y = .05) +
  annotate(geom = "text", label = "0.1%", x = 3.5, y = .05) +
  annotate(geom = "text", label = "2.1%", x = -2.5, y = .05) +
  annotate(geom = "text", label = "2.1%", x = 2.5, y = .05) +
  annotate(geom = "text", label = "13.6%", x = -1.5, y = .05) +
  annotate(geom = "text", label = "13.6%", x = 1.5, y = .05) +
  annotate(geom = "text", label = "34.1%", x = -.5, y = .05) +
  annotate(geom = "text", label = "34.1%", x = .5, y = .05) +
  scale_x_continuous(breaks = -4:4,
                     limits = c(-4, 4),
                     labels = c("",
                                "μ-3σ",
                                "μ-2σ",
                                "μ-σ",
                                "μ",
                                "μ+σ",
                                "μ+2σ",
                                "μ+3σ",
                                "")) +
  labs(x = "Значение", y = "Плотность вероятности")
```

Нормальное распределение и вероятности
:::

Конкретно с этими вероятностями мы работаем реже --- полезнее оказывается знать следующие:

$$
\begin{split}
&\mathbb{P} \big( X \in (\mu - \sigma, \mu + \sigma) \big) = 68.2\% \\
&\mathbb{P} \big( X \in (\mu - 2\sigma, \mu + 2\sigma) \big) = 95.6\% \\
&\mathbb{P} \big( X \in (\mu - 3\sigma, \mu + 3\sigma) \big) = 99.8\%
\end{split}
$${#eq-normdist-probs}

То есть

- в пределах одного стандартного отклонения от среднего значения лежит почти 70% значений --- это очень частотные значения;
- в пределах двух стандартных отклонений от среднего значения лежит 95% значений --- бо́льшая часть выборки;
- в пределах трех стандартных отклонений от среднего значения лежит практически 100% выборки --- практически вся выборка.

**Что нам это дает?**

- Во-первых, еще один способ определения выбросов (нехарактерных значений).
    - Например, так как за границы двух стандартных отклонений попадает всего 5% значений, мы можем считать, что эти значения --- нехарактерные и назвать их выбросами.     - Либо же можем быть более либеральными и сказать, что выбросы для нас --- значения, которые выходят за пределы трех стандартных отклонений. Главное обосновать, почему мы так считаем.
- Во-вторых, так мы можем определять статистические нормы.
    - Например, мы разрабатываем клинический опросник (типа MMPI какого-нибудь), и нам надо выяснить, какие значения на шкалах итогового балла будут являться «нормой», а какие «патологией». Мы собираем большую репрезентативную выборку (скажем, по России или по странам СНГ), строим распределение итогового балла по каждой из шкал опросника, и определяем границы нормы --- пусть это будет $\mu \pm 3\sigma$.
    - Теперь, когда новый респондент пройдет наш опросник, мы сможем сказать относительно его тестового балла, соответствует ли он нормативным границам или не соответствует.
    - Отмечу еще раз: *здесь мы говорим только о статистической норме* --- о таких значениях, которые чаще всего встречаются в выборке. Это *только один из возможных вариантов определения нормы*.

В целом, это всё, что нам надо знать про нормальное распределение.


## Стандартизация

Нормальных распределение существует много, и пример тому @fig-normdists-params. Однако среди всех нормальных распределений одно выделяется особо.

### Стандартное нормальное распределение

Нормальное распределение со средним $0$ и дисперсией $1$ называется **стандартным нормальным распределением (standard normal distribution)**, или $z$-распределением. Значения на шкале такого распределения называются $z$-значениями.

$$
z \thicksim \mathcal{N}(0, 1)
$$

Выглядит оно так:

:::{#fig-stnormdist}
```{r}
ggplot() +
  geom_function(fun = dnorm) +
  scale_x_continuous(breaks = -4:4,
                     limits = c(-4, 4)) +
  labs(x = "Значение", y = "Плотность вероятности")
```

Вид стандартного нормального распределения
:::

Разумеется, так как стандартное нормальное распределение является частным случаем нормального распределения, оно сохраняет все его свойства.

### Стандартизация

Часто возникает следующая задача: нам нужно сравнить, например, баллы по двум шкалам опросника, при этом размах баллов на шкале различный. Более того, сами распределения баллов по этим шкалам могут быть также различны.

Рассмотрим пример. Есть опросник «Trust in Artificial Intelligent Agents», который состоит из шести шкал — predictability, consistency, utility, faith, dependability и understanding. Возьмем две шкалы: predictability и understanding. Посмотрим, сколько вопросов вошло в каждую из шкал:

length(pr_items)
 [1] 10
length(un_items)
 [1] 12
Видим, что в шкале predictability 10 вопросов, а в шкале understanding 12. Понятно, что размах двух величин разный — сравнивать просто сырые баллы респондентов по двум шкалам друг с другом уже не выглядит осмысленно.

Посмотрим на распределения:



Видим, что разброс переменных разный, средние тоже разные — короче, все разное. Однако сравнивать каким-то образом хоцца. Для этого придумали стандартизацию.

Стандартизация — это такое преобразование исходной переменной, после которого среднее по переменной становится равно  
0
 , а стандартное отклонение —  
1
 . Таким образом, величина приводится к  
z
 -значениям.

Выполняется оно следующим образом:

$$
z_i = \frac{x_i - \overline X}{s_X},
$$
 
где $x_i$ --- значение исходной случайное величины, $\overline X$ --- выборочное среднее, $s_X$ --- выборочное стандартное отклонение.

Стандартизация состоит из двух операций:

- *центрирование* --- то, что происходит в числителе,
- *нормирование* --- то, что происходит в знаменателе.

Посмотрим на две операции отдельно на примере стандартизации какого-то нормального распределения.

:::{#fig-standartization-operations}
```{r}
tibble(
  x = seq(-5, 5, .05),
  "0_raw" = dnorm(x, mean = -1, sd = .5),
  "1_centred" = dnorm(x, mean = 0, sd = .5),
  "2_normed" = dnorm(x, mean = -1, sd = 1),
  "3_standed" = dnorm(x, mean = 0, sd = 1)
) %>% 
  pivot_longer(cols = -x) %>% 
ggplot(aes(x, value, color = name)) +
  geom_function(fun = dnorm, args = list(mean = -1, sd = .5),
                color = "black", linetype = "dotted") +
  geom_line() +
  facet_wrap(~name, ncol = 2,
             labeller = labeller(name = c("0_raw" = "Исходное распределение | N(-1, 0.25)",
                                   "1_centred" = "Центрированное распределение | N(0, 0.25)",
                                   "2_normed" = "Нормированное распределение | N(-1, 1)",
                                   "3_standed" = "Стандартизированное распределение | N(0, 1)"))) +
  geom_function(fun = dnorm, 
                color = "gray50", linetype = "dashed") +
  guides(color = "none") +
  scale_color_manual(values = c("0_raw" = "black",
                                "1_centred" = "royalblue",
                                "2_normed" = "springgreen3",
                                "3_standed" = "salmon")) +
  labs(x = "Значение", y = "Плотность вероятности")
```

Стандартизация, центрирование, нормирование. Пунктирная линия --- стандартное нормальное распределение, точечная линия --- исходное распределение.
:::



На рисунке [-@fig-standartization-operations] черной линией представлено исходное нормальное распределение $\mathcal{N}(-1,0.25)$ [слева сверху]. Если его центрировать, то оно подвинется вправо --- $x_i - \overline X$ --- и получится синее распределение $\mathcal{N}(0, 0.25)$ [справа сверху]. Если нормировать исходное распределение, то оно станет несколько шире --- $\frac{x_i}{s_X}$ --- как зелёное $\mathcal{N}(-1,1)$ [слева снизу]. А если осуществить обе операции вместе --- $\frac{x_i - \overline X}{s_X}$ --- это и будет стандартизация, и распределение совпадет сo стандартным нормальным распределением [справа снизу].

Важный момент: если распределение изначально было асимметричное, или бимодальное, или какое-либо еще «ненормальное», стандартизация не изменит его форму.

Например, вот.



Гистрограмма, конечно, изменилась существенно, но она имеет на это право, так как достаточно сильно чувствительна ко всяким случайным флуктуациям. А вот на графике плотности хорошо видно, что хотя распределение стало чуть шире и сдвинулось влево, форма распределения осталась прежней — есть два пика, есть хвостик справа.

А что же со шкалами опросника? Давайте стандартизируем:



Первое, что бросается в глаза — форма распределения, ну, прямо идентична [между стандартизированными и нестандартизированными величинами]. Шкалы исходных величин были различны [левые графики], шкалы стандартизированных величин — одинаковые [правые графики]. Обратите внимание на ось  
x
 .

### Интерпретация z-значений

Ну, хорошо, стандартизацию мы сделали — это не то чтобы очень сложно. Но как с этими числами дальше работать?

Здесь надо понять всего одну вещь: единица z-шкалы — это стандартное отклонение исходной переменной.

Конечно, z-шкала безразмерная, то есть не имеет единиц измерения (метры, секунды, года и др.). Однако единица — в смысле  
1
  — на этой шкале небессмысленна. Это мера типичности.

Посмотрим еще раз на картинку:



Обратим внимание вот на что:  
1
 ,  
2
 ,  
3
  и  
−
1
 ,  
−
2
 ,  
−
3
  находятся в тех самых точках, где у нас в другом нормальной распределении стандартные отклонения ( 
σ
 ,  
2
σ
 ,  
3
σ
 ). А мы помним вот что:

P
(
X
∈
(
μ
−
σ
,
μ
+
σ
)
)
=
68.2
%
 
P
(
X
∈
(
μ
−
2
σ
,
μ
+
2
σ
)
)
=
95.6
%
 
P
(
X
∈
(
μ
−
3
σ
,
μ
+
3
σ
)
)
=
99.8
%
 
То есть z-значения в интервале  
(
−
1
,
1
)
  — очень типичные, в интервалах  
(
−
2
,
−
1
)
  и  
(
1
,
2
)
  — тоже достаточно типичные, в интервалах  
(
−
3
,
−
2
)
  и  
(
2
,
3
)
  — менее типичные, но еще «нормальные», меньше  
−
3
  и больше  
3
  — очень нетипичные. Это и есть мера типичности.

Таким образом, с помощью z-значений мы можем понять, где находится наш испытуемый / респондент на нашем распределении.

Можно ли это понять без перевода значений в z-шкалу? Можно. Но неудобно. Надо держать в голове среднее, стандартное отклонение конкретного распределения, а если сравниваем респондентов по нескольким шкалам, то по две характеристики для каждой шкалы — в общем, много инфы. Трудно. А по z-значениям все сразу понятно.

Есть еще одно применение z-значений. От них можно перейти к значениям любой стандартной шкалы. Стандартных шкал существует много — Sten, Stanine, T-score, IQ-score… С ними вы познакомитесь в курсе психометрики. Они очень удобны для представления результатов тестов конечным пользователям. Однако для того, чтобы перейти к этим шкалам, сначала необходимо будет все равно перейти к z-значениям.
