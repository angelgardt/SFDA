# Нормальное распределение

```{r opts, echo=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, error = FALSE)
```

```{r}
library(tidyverse)
theme_set(theme_bw())
theme_update(legend.position = "bottom")
```

Мы научились описывать эмпирическое распределение переменных --- то есть распределение, которое получено на выборке. Но ведь в генеральной совокупности наши переменные тоже каким-то образом распределены. Уделим внимание и этому вопросу.

## Распределение признаков в генеральной совокупности {#normdist-pop-dist}

Можем ли мы знать наверняка, как распределен признак в генеральной совокупности?

Нет.

Однако мы можем предполагать некоторое теоретическое распределение признака. На основе чего мы можем сделать такое предположение? Конечно же, на основе собранных данных, то есть на основании выборки. Большой выборки. Очень большой выборки. Большого количества очень больших выборок.

Теоретических распределений существует много --- и они разные по форме, по параметрам, которыми они задаются, по величинам, которые ими можно описывать и т. д. Однако есть одно распределение, которое стало невероятно популярным и крайне широко используемым --- нормальное распределение. Его мы и будем рассматривать.

## Нормальное распределение {#normdist-normdist}

Тот факт, что некоторая величина распределена согласно закону **нормального распределения (normal distribution)**, записывается следующим образом:

$$
X \thicksim \mathcal{N}(\mu, \sigma^2)
$$

Здесь $X$ --- некоторая случайная величина (переменная), $\mathcal{N}$ --- обозначение нормального распределения, $\mu$ и $\sigma^2$ --- параметры нормального распределения.

### Параметры нормального распределения {#normdist-params}

Итак, в скобках указаны параметры распределения --- как можно видеть, их всего два. На самом деле, мы их уже хорошо знаем:

- $\mu$ --- это не что иное, как среднее,
- $\sigma^2$ --- дисперсия.

Эти два параметра входят в формулу, описывающую график функции плотности вероятности нормального распределения:

$$
f(x) = \frac{1}{\sigma \sqrt{2\pi}} e ^{- \frac{(x-\mu)^2}{2 \sigma^2}}, \quad x \in \mathbb{R}, \mu \in \mathbb{R}, \sigma \in \mathbb{R}_{>0}
$$

А сам график выглядит вот так:

:::{#fig-normdist-general}
```{r}
ggplot() +
  geom_function(fun = dnorm) +
  scale_x_continuous(breaks = -4:4,
                     limits = c(-4, 4),
                     labels = c("",
                                "μ-3σ",
                                "μ-2σ",
                                "μ-σ",
                                "μ",
                                "μ+σ",
                                "μ+2σ",
                                "μ+3σ",
                                "")) +
  labs(x = "Значение", y = "Плотность вероятности")
```

Общий вид нормального распределения
:::



И мы его уже тоже много раз видели.


:::{.callout-note}
#### Почему все так любят нормальное распределение?

- Его очень давно знают. Карл Фридрих Гаусс (1777–1855) исчерпывающе его исследовал, и теперь про это распределение известно всё. И ещё чуть более.
- Ряд статистических методов, называемых параметрическими, ~~требуют~~, чтобы распределение изучаемых переменных подчинялось нормальному закону.
    - Сейчас, строго говоря, это уже не совсем так. Появляются новые исследования и симуляции, показывающие, что это далеко не ключевое требование.
- С помощью нормального распределения определяют статистические нормы. Например, в образовательном тестировании, психодиагностике и иногда клинической практике.
- На основании нормального распределения рассчитывается стандартная ошибка среднего --- важная оценка в статистике.
- На основании нормального распределения, а точнее --- стандартной ошибки --- рассчитываются доверительные интервалы среднего --- одна из ключевых оценок в статистике.

:::


### Форма нормального распределения и параметры {#normdist-shape}

Очевидно, что раз у распределения есть какие-то параметры, значит они каким-то образом на него влияют. Не менее очевидно, что среднее $\mu$ будет задавать положение центра колокола на оси $x$, а дисперсия $\sigma^2$ --- ширину колокола. Ниже представлены несколько нормальных распределений c различными параметрами:

:::{#fig-normdists-params}
```{r}
tibble(
  x = seq(-10, 10, .05),
  "N(0, 1)" = dnorm(x, mean = 0, sd = 1),
  "N(−0.5, 0.25)" = dnorm(x, mean = -.5, sd = .5),
  "N(0, 2.25)" = dnorm(x, mean = 0, sd = 1.5),
  "N(1, 1)" = dnorm(x, mean = 1, sd = 1),
  "N(2, 4)" = dnorm(x, mean = 2, sd = 2)
) %>% 
  pivot_longer(cols = -x) %>% 
ggplot(aes(x, value, color = name)) +
  geom_line() +
  labs(x = "Значение", y = "Плотность вероятности",
       color = "Параметры")
```

Вид функции плотности нормального распределения при различных параметрах
:::


### Стандартные отклонения и вероятности {#normdist-probs}

Как уже упоминалось выше, нормальное распределение изучено вдоль и поперек. В том числе, посчитаны вероятности попадания значений в определенные интервалы. Вот они:

:::{#fig-normdist-probs}
```{r}
ggplot() +
  stat_function(fun = dnorm) +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(-1, 1),
                alpha = .5, fill = "royalblue") +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(-2, -1),
                alpha = .5, fill = "springgreen") +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(1, 2),
                alpha = .5, fill = "springgreen") +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(-3, -2),
                alpha = .5, fill = "gold") +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(2, 3),
                alpha = .5, fill = "gold") +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(-4, -3),
                alpha = .5, fill = "salmon") +
  stat_function(fun = dnorm,
                geom = "area",
                xlim = c(3, 4),
                alpha = .5, fill = "salmon") +
  geom_vline(xintercept = 0, linetype= "dashed") +
  geom_vline(xintercept = c(-4, -3, -2, -1, 1, 2, 3, 4), linetype= "dotted") +
  annotate(geom = "text", label = "0.1%", x = -3.5, y = .05) +
  annotate(geom = "text", label = "0.1%", x = 3.5, y = .05) +
  annotate(geom = "text", label = "2.1%", x = -2.5, y = .05) +
  annotate(geom = "text", label = "2.1%", x = 2.5, y = .05) +
  annotate(geom = "text", label = "13.6%", x = -1.5, y = .05) +
  annotate(geom = "text", label = "13.6%", x = 1.5, y = .05) +
  annotate(geom = "text", label = "34.1%", x = -.5, y = .05) +
  annotate(geom = "text", label = "34.1%", x = .5, y = .05) +
  scale_x_continuous(breaks = -4:4,
                     limits = c(-4, 4),
                     labels = c("",
                                "μ-3σ",
                                "μ-2σ",
                                "μ-σ",
                                "μ",
                                "μ+σ",
                                "μ+2σ",
                                "μ+3σ",
                                "")) +
  labs(x = "Значение", y = "Плотность вероятности")
```

Нормальное распределение и вероятности
:::

Конкретно с этими вероятностями мы работаем реже --- полезнее оказывается знать следующие:

$$
\begin{split}
&\mathbb{P} \big( X \in (\mu - \sigma, \mu + \sigma) \big) = 68.2\% \\
&\mathbb{P} \big( X \in (\mu - 2\sigma, \mu + 2\sigma) \big) = 95.6\% \\
&\mathbb{P} \big( X \in (\mu - 3\sigma, \mu + 3\sigma) \big) = 99.8\%
\end{split}
$${#eq-normdist-probs}

То есть

- в пределах одного стандартного отклонения от среднего значения лежит почти 70% значений --- это очень частотные значения;
- в пределах двух стандартных отклонений от среднего значения лежит 95% значений --- бо́льшая часть выборки;
- в пределах трех стандартных отклонений от среднего значения лежит практически 100% выборки --- практически вся выборка.

**Что нам это дает?**

- Во-первых, еще один способ определения выбросов (нехарактерных значений).
    - Например, так как за границы двух стандартных отклонений попадает всего 5% значений, мы можем считать, что эти значения --- нехарактерные и назвать их выбросами.     - Либо же можем быть более либеральными и сказать, что выбросы для нас --- значения, которые выходят за пределы трех стандартных отклонений. Главное обосновать, почему мы так считаем.
- Во-вторых, так мы можем определять статистические нормы.
    - Например, мы разрабатываем клинический опросник (типа MMPI какого-нибудь), и нам надо выяснить, какие значения на шкалах итогового балла будут являться «нормой», а какие «патологией». Мы собираем большую репрезентативную выборку (скажем, по России или по странам СНГ), строим распределение итогового балла по каждой из шкал опросника, и определяем границы нормы --- пусть это будет $\mu \pm 3\sigma$.
    - Теперь, когда новый респондент пройдет наш опросник, мы сможем сказать относительно его тестового балла, соответствует ли он нормативным границам или не соответствует.
    - Отмечу еще раз: *здесь мы говорим только о статистической норме* --- о таких значениях, которые чаще всего встречаются в выборке. Это *только один из возможных вариантов определения нормы*.

В целом, это всё, что нам надо знать про нормальное распределение.


## Стандартизация {#normdist-stand}

Нормальных распределение существует много, и пример тому @fig-normdists-params. Однако среди всех нормальных распределений одно выделяется особо.

### Стандартное нормальное распределение {#normdist-stnormdist}

Нормальное распределение со средним $0$ и дисперсией $1$ называется **стандартным нормальным распределением (standard normal distribution)**, или $z$-распределением. Значения на шкале такого распределения называются $z$-значениями.

$$
z \thicksim \mathcal{N}(0, 1)
$$

Выглядит оно так:

:::{#fig-stnormdist}
```{r}
ggplot() +
  geom_function(fun = dnorm) +
  scale_x_continuous(breaks = -4:4,
                     limits = c(-4, 4)) +
  labs(x = "Значение", y = "Плотность вероятности")
```

Вид стандартного нормального распределения
:::

Разумеется, так как стандартное нормальное распределение является частным случаем нормального распределения, оно сохраняет все его свойства.

### Стандартизация {#normdist-standizaion}

Часто возникает следующая задача: нам нужно сравнить, например, баллы по шкалам двух опросников, при этом разброс баллов на шкале различный. Более того, сами распределения баллов по этим шкалам могут быть также различны.

Рассмотрим следующий пример. Мы дали респондентам два опросника на измерение личностных черт --- BFI-2 (по модели «Большая пятерка») и SD3 (по модели «Тёмная триада»). Допустим, что мы хотим сравнить баллы некоторого респондента по шкалам Нейротизм из BFI-2 и Нарциссизм из SD3. Вопрос будет простой: *по какой из двух шкал респондент набрал больший балл*?

Данные говорят, что этот респондент набрал $31$ балл по обеим шкалам. Собственно, а в чём тогда вопрос? Ясно же, что баллов набрано по обеим шкалам одинаково. Или всё же нет…

Шкалы BFI-2 состоят из двенадцати пунктов, а шкалы SD3 --- из девяти. И даже учитывая, что отвечали респонденты в обоих случаях по пятибалльной шкале Ликерта, суммарный балл по шкалам опросника будет иметь разные пределы изменчивости (@fig-survey-sample). Описательные статистики --- например, среднее и стандартное отклонение --- будут также различны (@tbl-survey-means).

:::{#fig-survey-sample}
```{r}
set.seed(123)
SD3_n <- 9
BFI2_n <- 12
tibble(id = 1:500,
       BFI2_NEU = rnorm(500, mean = 3*BFI2_n, sd = 10) %>% round(),
       SD3_NAR = rnorm(500, mean = 3*SD3_n, sd = 7) %>% round()
       ) %>% 
  filter(BFI2_NEU >= BFI2_n & BFI2_NEU <= BFI2_n*5,
         SD3_NAR >= SD3_n & SD3_NAR <= SD3_n*5) -> bfisd

bfisd %>% 
  pivot_longer(cols = -id) %>% 
  ggplot(aes(value)) +
  geom_histogram() +
  facet_wrap(~name) +
  scale_x_continuous(breaks = seq(10, 60, 10)) +
  labs(x = "Балл", y = "Количество")
```

Распределения баллов по шкалам опросников. `BFI2_NEU` --- шкала Нейротизм опросника BFI-2, `SD3_NAR` --- шкала Нарциссизм опросника SD3.
:::

:::{#tbl-survey-means}
|Шкала|Количество пунктов|Количество респондентов|Среднее|Стандартное отклонение|
|:---:|:---:|:---:|:---:|:---:|
|`BFI2_NEU`|12|`r nrow(bfisd)`|`r bfisd$BFI2_NEU %>% mean() %>% round(2)`|`r bfisd$BFI2_NEU %>% sd() %>% round(2)`|
|`SD3_NAR`|9|`r nrow(bfisd)`|`r bfisd$SD3_NAR %>% mean() %>% round(2)`|`r bfisd$SD3_NAR %>% sd() %>% round(2)`|

Описательные статистики распределений баллов по шкалам опросников. `BFI2_NEU` --- шкала Нейротизм опросника BFI-2, `SD3_NAR` --- шкала Нарциссизм опросника SD3.
:::

В силу разного разброса значение одни и те же значения будут находится в разных частях распределений. В частности, внимательно присмотревшись к описательным статистикам (@tbl-survey-means) можно заметить, что значение $31$ меньше среднего для шкалы `BFI2-NEU` и больше среднего для шкалы `SD3_NAR`. Это, безусловно, полезная информация, однако хотелось бы найти какой-то единый способ для сравнения значений из различных распределений. Такой способ есть --- нам необходимо *стандартизировать* случайный величины.

:::{#def-standartization}
**Стандартизация** --- преобразование случайной величины, в результате которого среднее её распределения становится равным $0$, а стандартное отклонение --- $1$.

$$
z_i = \frac{x_i - \overline X}{s_X},
$${#eq-standartization}

где $z_i$ --- значение стандартизированной случайной величины, $x_i$ --- значение исходной случайной величины, $\overline X$ --- среднее исходной случайной величины, $s_X$ --- стандартное отклонение исходной случайной величины.

:::

Полученные стандартизированные значения также часто называют $z$-значениями. В этом есть довольно понятный смысл, ведь если изначально переменная была распределена нормально --- $X \thicksim \mathcal{N}(\mu, \sigma^2)$ --- то стандартизированная случайная величина будет подчиняться стандартному нормальному распределению --- $z \thicksim \mathcal{N}(0, 1)$.

Посмотрим на процедуру стандартизации детально, благо деталей всего две --- числитель и знаменатель (см. @eq-standartization). Стандартизация состоит из двух операций:

- *центрирование* --- то, что происходит в числителе --- обращает среднее в ноль (см. @prp-mean1),
- *нормирование* --- то, что происходит в знаменателе --- обращает стандартное отклонение в единицу (см. @cor-sd2).

Посмотрим на две операции отдельно на примере стандартизации какого-то нормального распределения.

:::{#fig-standartization-operations}
```{r}
tibble(
  x = seq(-5, 5, .05),
  "0_raw" = dnorm(x, mean = -1, sd = .5),
  "1_centred" = dnorm(x, mean = 0, sd = .5),
  "2_normed" = dnorm(x, mean = -1, sd = 1),
  "3_standed" = dnorm(x, mean = 0, sd = 1)
) %>% 
  pivot_longer(cols = -x) %>% 
ggplot(aes(x, value, color = name)) +
  geom_function(fun = dnorm, args = list(mean = -1, sd = .5),
                color = "black", linetype = "dotted") +
  geom_line() +
  facet_wrap(~name, ncol = 2,
             labeller = labeller(name = c("0_raw" = "Исходное распределение | N(-1, 0.25)",
                                   "1_centred" = "Центрированное распределение | N(0, 0.25)",
                                   "2_normed" = "Нормированное распределение | N(-1, 1)",
                                   "3_standed" = "Стандартизированное распределение | N(0, 1)"))) +
  geom_function(fun = dnorm, 
                color = "gray50", linetype = "dashed") +
  guides(color = "none") +
  scale_color_manual(values = c("0_raw" = "black",
                                "1_centred" = "royalblue",
                                "2_normed" = "springgreen3",
                                "3_standed" = "salmon")) +
  labs(x = "Значение", y = "Плотность вероятности")
```

Стандартизация, центрирование, нормирование. Пунктирная линия --- стандартное нормальное распределение, точечная линия --- исходное распределение.
:::

На рисунке [-@fig-standartization-operations] черной линией представлено исходное нормальное распределение $\mathcal{N}(-1,0.25)$ [слева сверху]. Если его центрировать, то оно подвинется вправо --- $x_i - \overline X$ --- и получится синее распределение $\mathcal{N}(0, 0.25)$ [справа сверху]. Если нормировать исходное распределение, то оно станет несколько шире --- $\frac{x_i}{s_X}$ --- как зелёное $\mathcal{N}(-1,1)$ [слева снизу]. А если осуществить обе операции вместе --- $\frac{x_i - \overline X}{s_X}$ --- это и будет стандартизация, и распределение совпадет сo стандартным нормальным распределением [справа снизу].

::::{.callout-important}
#### Стандартизация не меняет форму распределения!

В реальной ситуации никто не может нам гарантировать, что выборочное распределение обязательно будет нормальным. Вообще-то на выборке может случится всё, что угодно.

Иногда можно услышать такое утверждение: *«стандартизация приводит произвольное распределение к стандартному нормальному»* --- что совершеннейшая неправда. Вернитесь к определению [-@def-standartization] и убедить, что там нет ни слова про нормальность распределений.

Рассмотрим пару примеров. Пусть исходное распределение асимметрично (@fig-asym-stand). Стандартизировав его, мы получим распределение со средним $0$ и стандартным отклонением $1$, однако его форма совершенно не изменилась.

:::{#fig-asym-stand}
```{r}
set.seed(202)
tibble(raw = rbeta(200, 1, 3),
       scaled = scale(raw)) %>% 
  pivot_longer(cols = everything(),
               values_to = "x") %>% 
  ggplot(aes(x)) +
  geom_histogram(aes(y = after_stat(density)),
                 fill = "gray50") +
  geom_density(linewidth = 2) +
  facet_wrap(~name, 
             labeller = labeller(name = c(
               "raw" = "Исходное распределение",
               "scaled" = "Стандартизированное распределение")),
             scales = "free") +
  labs(x = "Значения", y = "Плотность")
```

Стандартизация асимметричного распределения
:::


Теперь возьмем бимодальное распределение, и пронаблюдаем ровно ту же ситуацию --- форма распределения вновь не изменилась (@fig-bimod-stand).

:::{#fig-bimod-stand}
```{r}
set.seed(202)
tibble(raw = c(
  rnorm(130, 20, 4),
  rnorm(70, 5, 4)
  ),
       scaled = scale(raw)) %>% 
  pivot_longer(cols = everything(),
               values_to = "x") %>% 
  ggplot(aes(x)) +
  geom_histogram(aes(y = after_stat(density)),
                 fill = "gray50") +
  geom_density(linewidth = 2) +
  facet_wrap(~name, 
             labeller = labeller(name = c(
               "raw" = "Исходное распределение",
               "scaled" = "Стандартизированное распределение")),
             scales = "free") +
  labs(x = "Значения", y = "Плотность")
```

Стандартизация бимодального распределения
:::

::::

Вернёмся же, наконец, к нашей исходной задаче: мы собирались ответить на вопрос *по какой из двух шкал респондент набрал больший балл*, при этом данные нам говорят, что респондент набрал $31$ балл по обеим шкалам. Найдём стандартизированное значение по формуле [-@eq-standartization] взяв значения среднего и стандартного отклонения из таблицы [-@tbl-survey-means]. Получим

$$
\begin{split}
z_\text{BFI2_NEU} &= \frac{31 - 36.34}{9.41} \approx -0.57 \\
z_\text{SD3_NAR} &= \frac{31 - 29.94}{6.95} \approx 0.58
\end{split}
$$

Таким образом, мы можем сказать, что наш респондент набрал более высокий балл по шкале нарциссизма.



### Интерпретация z-значений {#normdist-stand-interpret}

Хорошо, мы сделали стандартизацию --- довольно простая процедура. Но как с этими числами дальше работать? Можно ли из них выжать больше смысла, чем просто сравнение?

Здесь надо понять всего одну вещь:

:::{.callout-tip appearance="minimal"}
единица $z$-шкалы --- это стандартное отклонение исходной переменной
:::

Конечно, $z$-шкала безразмерная, то есть не имеет единиц измерения (метры, секунды, года и др.). Однако единица --- в смысле $1$ --- на этой шкале небессмысленна. Это мера типичности.

Взглянем ещё раз на рисунок [-@fig-stnormdist]. Обратим внимание вот на что: $1$, $2$ $3$ и $-1$, $-2$, $-3$ находятся в тех самых точках, где в другом нормальном распределении находятся стандартные отклонения ($\mu \pm \sigma$, $\mu \pm 2\sigma$, $\mu \pm 3\sigma$). Мы также знаем, что этим диапазонам соответствуют вероятности, с которыми значения случайных величин оказываются в этих диапазонах (@eq-normdist-probs).


То есть мы можем говорить, что $z$-значения

- в интервале $(-1,1)$ --- очень типичные, поскольку лежат в пределах одного стандартного отклонения от среднего,
- в интервалах $(-2, -1)$ и $(1,2)$ --- тоже достаточно типичные, поскольку лежат в пределах двух стандартных отклонений от среднего,
- в интервалах $(-3, -2)$ и $(2,3)$ --- менее типичные, но еще «нормальные», так как лежат в пределах трёх стандартных отклонений от среднего,
- меньшие $-3$ и большие $3$ --- очень нетипичные, так как выходят за пределы трёх стандартных отклонений от среднего.

Это и есть мера типичности.

Таким образом, с помощью $z$-значений мы можем быстро понять, где находится испытуемый или респондент на имеющемся распределении.

Можно ли это понять без перевода значений в $z$-шкалу? Можно. Но неудобно. Надо держать в голове среднее и стандартное отклонение конкретного распределения, а если сравниваем респондентов по нескольким шкалам, то по две характеристики для каждой шкалы, постоянно пересчитывая диапазоны --- в общем, слишком много инфы. Трудно.


### Применение стандартизации {#normist-stand-usage}

Помимо обсужденного выше, есть ещё одно применение $z$-значений. От них можно перейти к значениям любой другой стандартной шкалы, выполнив преобразование, обратное [-@eq-standartization]. Стандартных шкал существует много --- Sten, Stanine, T-score, IQ-score… С ними вы познакомитесь в курсе психометрики. Они оказываются очень удобными для представления результатов тестов конечным пользователям. Однако для того, чтобы перейти к этим шкалам, сначала все равно будет необходимо перейти к $z$-значениям.

Кроме того, стандартизация также используется в некоторых статистических методах, когда нам необходимо уравнять дисперсии переменных. Поскольку после стандартизации дисперсия переменной всегда будет равна единице, такая процедура отлично подходит для этой задачи.
