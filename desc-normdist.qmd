# Нормальное распределение

Мы научились описывать эмпирическое распределение переменных — то есть распределение, которое получено на выборке. Но ведь в генеральной совокупности наши переменные тоже каким-то образом распределены! Уделим внимание и этому вопросу.

5.1 Распределение признаков в генеральной совокупности
Можем ли мы знать наверняка, как распределен признак в генеральной совокупности? Нет. Однако мы можем предполагать некоторое теоретическое распределение признака. На основе чего мы можем сделать такое предположение? Конечно же, на основе собранных данных, то есть на основании выборки. Большой выборки. Очень большой выборки. Большого количества очень больших выборок.

Теоретических распределений существует много — и они разные по форме, по параметрам, которыми они описываются, по величинам, которые ими можно описывать и т.д. Однако есть одно распределение, которое стало невероятно популярным и крайне широко используемым — нормальное распределение. Его мы и будем рассматривать.

5.2 Нормальное распределение
Тот факт, что некоторая величина распределена согласно закону нормального распределения, записывается следующим образом:

X
∼
N
(
μ
,
σ
2
)
 

Здесь  
X
  — некоторая случайная величина,  
N
  — обозначение нормального распределения,  
μ
  и  
σ
2
  — параметры нормального распределения.

5.2.1 Параметры нормального распределения
Итак, в скобках указаны параметры распределения — как можно видеть, их всего два. На самом деле, мы их уже хорошо знаем:  
μ
  — это не что иное, как среднее, а  
σ
2
  — дисперсия. Эти два параметра входят в формулу, описывающую график функции плотности вероятности нормального распределения:

1
σ
√
2
π
e
−
(
x
−
μ
)
2
2
σ
2
x
∈
R
,
μ
∈
R
,
σ
∈
R
>
0

А сам график выглядит вот так:



И мы его уже тоже много раз видели.

5.2.2 Почему все так любят нормальное распределение?
Его очень давно знают. Карл Фридрих Гаусс (1777–1855) исчерпывающе его исследовал, и теперь про это распределение известно всё. И ещё чуть более.
Ряд статистических методов, называемых параметрическими, требуют, чтобы распределение изучаемых переменных подчинялось нормальному закону.
Сейчас, строго говоря, это уже не совсем так. Появляются новые исследования и симуляции, показывающие, что это не ключевое требование, однако актуальность нормального распределения от этого не гаснет.
С помощью нормального распределения определяют статистические нормы. Например, в образовательном тестировании, психодиагностике или иногда клинической практике.
На основании нормального распределения рассчитывается стандартная ошибка среднего — важная оценка в статистике.
На основании нормального распределения, а точнее — стандартной ошибки — рассчитываются доверительные интервалы среднего — одна из ключевых оценок в статистике.
5.3 Форма нормального распределения и параметры
Очевидно, что раз у распределения есть какие-то параметры, значит они каким-то образом на него влияют. Не менее очевидно, что среднее  
μ
  будет задавать положение центра колокола на оси  
x
 , а дисперсия  
σ
2
  — ширину колокола. Ниже представлены несколько нормальных распределений в различными параметрами:



5.4 Стандартные отклонения и вероятности
Какиуже упоминалось выше, нормальное распределение изучено вдоль и поперек. В том числе, посчитаны вероятности попадания значений в определенные и интервалы. Вот они:



Конкретно с этими вероятностями мы работаем реже — полезнее оказывается знать следующие:

P
(
X
∈
(
μ
−
σ
,
μ
+
σ
)
)
=
68.2
%
 
P
(
X
∈
(
μ
−
2
σ
,
μ
+
2
σ
)
)
=
95.6
%
 
P
(
X
∈
(
μ
−
3
σ
,
μ
+
3
σ
)
)
=
99.8
%
 
То есть

в пределах одного стандартного отклонения от среднего значения лежит почти 70% значений — это очень частотные значения;
в пределах двух стандартных отклонений от среднего значения лежит 95% значений — бо́льшая часть выборки;
в пределах трех стандартных отклонений от среднего значения лежит практически 100% выборки — то есть вся выборка.
Что нам это дает?

Во-первых, еще один способ определения выбросов (нехарактерных значений). Например, так как за границы двух стандартных отклонений попадает всего 5% значений, мы можем считать, что эти значения — нехарактерные и назвать и выбросами. Либо же можем быть более либеральными и сказать, что выбросы для нас — значения, которые выходят за пределы трех стандартных отклонений. Главное обосновать, почему мы так считаем.

Во-вторых, так мы можем определять статистические нормы. Например, мы разрабатываем клинический опросник (типа MMPI какого-нибудь), и нам надо выяснить, какие значения на шкалах итогового балла будут являться «нормой», а какие «патологией». Мы собираем огромную выборку (скажем, по России или по странам СНГ), строим распределение итогового балла по каждой из шкал опросника, и определяем границы нормы — пусть это будет  
μ
±
3
σ
 . Теперь, когда новый респондент пройдет наш опросник, мы сможем сказать относительно его тестового балла, соответствует ли он нормативным границам или не соответствует.

Отмечу еще раз: здесь мы говорим только о статистической норме — о таких значениях / показателях, которые чаще всего встречаются в выборке. Это только один из возможных вариантов определения нормы.

В целом, это всё, что нам надо знать про нормальное распределение.


6 Стандартизация
Полезная вещь.

6.1 Стандартное нормальное распределение
Есть одно нормальное распределение, которые выделяется среди остальных — это нормальное распределение со средним  
0
  и дисперсией (стандартным отклонение)  
1
 . Оно называется  
z
 -распределение, а значения на шкале такого распределния называются  
z
 -значениями.

z
∼
N
(
0
,
1
)
 

Выглядит оно так:



Разумеется, так как стандартное нормальное распределение является частным случаем нормального распредления, оно сохраняет все его свойства.

6.2 Стандартизация
Часто возникает следующая задача: нам нужно сравнить, например, баллы по двум шкалам опросника, при этом размах баллов на шкале различный. Более того, сами распределения баллов по этим шкалам могут быть также различны.

Рассмотрим пример. Есть опросник «Trust in Artificial Intelligent Agents», который состоит из шести шкал — predictability, consistency, utility, faith, dependability и understanding. Возьмем две шкалы: predictability и understanding. Посмотрим, сколько вопросов вошло в каждую из шкал:

length(pr_items)
## [1] 10
length(un_items)
## [1] 12
Видим, что в шкале predictability 10 вопросов, а в шкале understanding 12. Понятно, что размах двух величин разный — сравнивать просто сырые баллы респондентов по двум шкалам друг с другом уже не выглядит осмысленно.

Посмотрим на распределения:



Видим, что разброс переменных разный, средние тоже разные — короче, все разное. Однако сравнивать каким-то образом хоцца. Для этого придумали стандартизацию.

Стандартизация — это такое преобразование исходной переменной, после которого среднее по переменной становится равно  
0
 , а стандартное отклонение —  
1
 . Таким образом, величина приводится к  
z
 -значениям.

Выполняется оно следующим образом:

z
=
x
−
¯
x
s
,
 

где  
x
  — значение исходной случайное величины,  
¯
x
  — выборочное среднее,  
s
  — выборочное стандартное отклонение.

Стандартизация состоит из двух операций:

центрирование — то, что происходит в числителе,
нормирование — то, что происходит в знаменателе.
Посмотрим на две операции отдельно на примере стандартизации какого-то нормального распределения.



На рисунке черной линией представлено исходное нормальное распределение  
N
(
−
0.5
,
0.25
)
 . Если его центрировать, то оно подвинется вправо —  
x
−
¯
x
  — и получится синее распределение  
N
(
0
,
0.25
)
 . Если нормировать исходное распределение, то оно станет несколько шире —  
x
s
  — как зеленое  
N
(
−
0.5
,
1
)
 . А если осуществить обе операции вместе —  
x
−
¯
x
s
  — это и будет стандартизация, и распределение совпадет с красным (стандартным нормальным распределением).

Важный момент: если распределение изначально было асимметричное, или бимодальное, или какое-либо еще «ненормальное», стандартизация не изменит его форму.

Например, вот.



Гистрограмма, конечно, изменилась существенно, но она имеет на это право, так как достаточно сильно чувствительна ко всяким случайным флуктуациям. А вот на графике плотности хорошо видно, что хотя распределение стало чуть шире и сдвинулось влево, форма распределения осталась прежней — есть два пика, есть хвостик справа.

А что же со шкалами опросника? Давайте стандартизируем:



Первое, что бросается в глаза — форма распределения, ну, прямо идентична [между стандартизированными и нестандартизированными величинами]. Шкалы исходных величин были различны [левые графики], шкалы стандартизированных величин — одинаковые [правые графики]. Обратите внимание на ось  
x
 .

6.3 Интерпретация z-значений
Ну, хорошо, стандартизацию мы сделали — это не то чтобы очень сложно. Но как с этими числами дальше работать?

Здесь надо понять всего одну вещь: единица z-шкалы — это стандартное отклонение исходной переменной.

Конечно, z-шкала безразмерная, то есть не имеет единиц измерения (метры, секунды, года и др.). Однако единица — в смысле  
1
  — на этой шкале небессмысленна. Это мера типичности.

Посмотрим еще раз на картинку:



Обратим внимание вот на что:  
1
 ,  
2
 ,  
3
  и  
−
1
 ,  
−
2
 ,  
−
3
  находятся в тех самых точках, где у нас в другом нормальной распределении стандартные отклонения ( 
σ
 ,  
2
σ
 ,  
3
σ
 ). А мы помним вот что:

P
(
X
∈
(
μ
−
σ
,
μ
+
σ
)
)
=
68.2
%
 
P
(
X
∈
(
μ
−
2
σ
,
μ
+
2
σ
)
)
=
95.6
%
 
P
(
X
∈
(
μ
−
3
σ
,
μ
+
3
σ
)
)
=
99.8
%
 
То есть z-значения в интервале  
(
−
1
,
1
)
  — очень типичные, в интервалах  
(
−
2
,
−
1
)
  и  
(
1
,
2
)
  — тоже достаточно типичные, в интервалах  
(
−
3
,
−
2
)
  и  
(
2
,
3
)
  — менее типичные, но еще «нормальные», меньше  
−
3
  и больше  
3
  — очень нетипичные. Это и есть мера типичности.

Таким образом, с помощью z-значений мы можем понять, где находится наш испытуемый / респондент на нашем распределении.

Можно ли это понять без перевода значений в z-шкалу? Можно. Но неудобно. Надо держать в голове среднее, стандартное отклонение конкретного распределения, а если сравниваем респондентов по нескольким шкалам, то по две характеристики для каждой шкалы — в общем, много инфы. Трудно. А по z-значениям все сразу понятно.

Есть еще одно применение z-значений. От них можно перейти к значениям любой стандартной шкалы. Стандартных шкал существует много — Sten, Stanine, T-score, IQ-score… С ними вы познакомитесь в курсе психометрики. Они очень удобны для представления результатов тестов конечным пользователям. Однако для того, чтобы перейти к этим шкалам, сначала необходимо будет все равно перейти к z-значениям.
