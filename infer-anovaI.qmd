# Однофакторный дисперсионный анализ {#infer-anovaI}

```{r opts, echo=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, error = FALSE)
```

```{r}
library(tidyverse)
theme_set(theme_bw())
theme_update(legend.position = "bottom")
library(latex2exp)
```

Мы разобрались --- по крайне мере, в первом приближении --- как анализировать связи между количественными переменными. Что же делать, если у нас одна из переменных категориальная?

## Эксперимент {#anovaI-exp}

Одна категориальная и одна количественная переменная --- это типичнейшая ситуация для эксперимента. Допустим мы изучаем эмоциональные штуки. Нам интересно сравнить скорость распознавания нейтральных и эмоционально окрашенных слов. Мы решили предложить испытуемому задачу лексического решения --- предъявляем ему слова и не-слова. Испытуемому необходимо ответить, является ли представленный набор символов словом. Пусть ответ даётся нажатием на клавишу «→», если набор символов является словом, и «←», если таковым не является.

Сколько нам нужно экспериментальных условий? Кажется, что три --- нейтральные стимулы, стимулы с положительной валентностью и стимулы с отрицательной валентностью. И пусть у нас три группы испытуемых --- каждая группа проходит одно экспериментальное условие. Не-слова будем считать филлерами и не будем анализировать пробы с ними.

Итого,

- ЗП --- время реакции
- НП --- тип стимула (валентность): три уровня — нейтральные, негативные, позитивные.

Ну, дизайн кратко описан. Предположим, что мы корректно сформировали выборку и собрали некоторые данные. Посмотрим, что в них будет происходить.

## Структура изменчивости данных {#anovaI-varstruct}

Мы говорили, что основные характеристики статистических данных --- неопределённость и вариативность. И эта вариативность (изменчивость) имеет определенную структуру.

Мы также говорили, что основная мера вариативности --- это дисперсия. Это верно, однако сейчас чуть замедлимся, и сначала поговорим о суммах квадратов. Ну, то же самое, что дисперсия, только на степени свободы пока не делим.

Итак, наши данные выглядят как-то так:

:::{#fig-anova-scheme-data}
![](img/infer/ow-anova-scheme-data.jpg)

Схема визуализации данных
:::

По оси $x$ наша независимая экспериментальная переменная --- категориальная, как можно наблюдать. Видно, что у нас три экспериментальные группы. По оси $y$ наша зависимая экспериментальная переменная --- количественная, как можно наблюдать. Видно, что в каждой группе наблюдений есть какой-то разброс.

Прежде всего, есть **общая изменчивость**, или **общая сумма квадратов (total sum of squares, TSS)** которая складывается из [сумм квадратов] отклонений от общего среднего:

$$
\text{TSS} = \sum_{i=1}^n (\overline y - y_i)^2, \quad 
\text{df}_\text{TSS} = n-1
$$

:::{#fig-anova-scheme-tss}
![](img/infer/ow-anova-scheme-tss.jpg)

Общая сумма квадратов (TSS)
:::

Есть **факторная (объясненная) изменчивость (explained sum of squares, ESS)** --- это отклонения внутригрупповых средних от общего среднего с учетом количества наблюдений в каждой из групп:

$$
\text{ESS} = \sum_{j=1}^p n_j \cdot (\overline y - \overline y_j)^2, \quad
\text{df}_\text{ESS} = p-1
$$

:::{#fig-anova-scheme-ess}
![](img/infer/ow-anova-scheme-ess.jpg)

Объясненная сумма квадратов (ESS)
:::

Факторная она, потому что то, что мы называем в эксперименте *независимой переменной*, в дисперсионном анализе называется *фактором*. Объясненная, потому что она *объясняется действием фактора* (независимой переменной).

И есть какая-то изменчивость, которая остается необъясненной --- **случайная (остаточная) изменчивость (residual sum of squares, RSS)**:

$$
\text{RSS} = \sum_{j=1}^p \sum_{i=1}^{n_j} (\overline y_j - y_{ji})^2, \quad
\text{df}_\text{RSS} = n-p
$$
 
:::{#fig-anova-scheme-rss}
![](img/infer/ow-anova-scheme-rss.jpg)

Остаточная сумма квадратов (RSS)
:::

Очевидно, что

$$
\text{TSS} = \text{ESS} + \text{RSS}
$$


## Зачем нужен дисперсионный анализ? {#anovaI-why}

Что мы, собственно, выясняем? Вернемся к задумке эксперимента --- мы хотим узнать, связана скорость опознания слова с его эмоциональной окраской. Как эта связь будет выглядеть в данных?

- Если связь существует, то объяснённая изменчивость (ESS) будет больше, чем случайная изменчивость (RSS).
    - Обратите внимание на рисунок [-@fig-anova-mseffect-yes]. Основной вклад в вариативность данных вносит факторная изменчивость --- средние по группам сильно отстают от общего среднего, в то время как наблюдения от своих групповых средних лежат в целом достаточно близко.
- Если связь существует, то объяснённая изменчивость (ESS) будет меньше, чем случайная изменчивость (RSS), или же равна её.
    - Обратите внимание на рисунок [-@fig-anova-mseffect-no]. Вклад факторной изменчивости в вариативность данных крайне мал --- средние по группам очень близки к общему среднему, в то время как отклонения наблюдений от своих групповых средних составляют бо́льшую часть вариативности данных.


:::{#fig-anova-mseffect layout-ncol=2}

![Наличие эффекта фактора](img/infer/ow-anova-scheme-var.jpg){#fig-anova-mseffect-yes}

![Отсутствие эффекта фактора](img/infer/ow-anova-scheme-novar.jpg){#fig-anova-mseffect-no}

Структура изменчивости в ситуациях отсутствия и наличия эффекта группирующего фактора
:::


То есть, для того, чтобы сделать вывод о том, есть ли связь между фактором и зависимой переменной, нам нужно сопоставить факторную изменчивость и остаточную изменчивость.

Теперь у нас есть все, чтобы поговорить о тестирования статистической значимости фактора. Именно этим и занимается **дисперсионный анализ** (**an**alysis **o**f **va**riance, **ANOVA**).


## Тестирование значимости фактора {#anovaI-ftest}

Поскольку у нас может быть разное количество наблюдений в группах и различное количество групп, лучше сравнивать не саму изменчивость --- суммы квадратов --- а среднюю изменчивость --- средние квадраты. Они вычисляются так:

$$
\text{MS}_\text{t} = \frac{\text{TSS}}{\text{df}_\text{TSS}} = 
\frac{\sum_{i=1}^n (\overline y - y_i)^2}{n-1}
$$

$$
\text{MS}_\text{e} = \frac{\text{ESS}}{\text{df}_\text{ESS}} = 
\frac{\sum_{j=1}^p n_j \cdot (\overline y - \overline y_i)^2}{p-1}
$$

$$
\text{MS}_\text{r} = \frac{\text{RSS}}{\text{df}_\text{RSS}} = 
\frac{\sum_{j=1}^p \sum_{i=1}^{n_j} (\overline y_j - y_{ij})^2}{n-p}
$$

Ну, $\text{MS}_\text{t}$ легко узнать --- это буквально дисперсия, $\text{MS}_\text{e}$ и $\text{MS}_\text{r}$ тоже на неё похожи, но их принято называть **средними квадратами**.

$\text{MS}_\text{e}$ и $\text{MS}_\text{r}$ помогают протестировать значимость фактора:

- Если зависимость между фактором и целевой переменной есть, то $\text{MS}_\text{e} > \text{MS}_\text{r}$
- Если искомой зависимости нет, то $\text{MS}_\text{e} \approx \text{MS}_\text{r}$ или даже $\text{MS}_\text{e} < \text{MS}_\text{r}$.



В терминах статистических гипотез это будет выглядеть так:

$$
\begin{aligned}
H_0&: \mu_1 = \mu_2 = \ldots = \mu_p \\
H_1&: \exists i, j : \mu_i \neq \mu_j
\end{aligned}
$$

Для тестирования этой гипотезы придумали вот такую статистику:

$$
F = \frac{\text{MS}_\text{e}}{\text{MS}_\text{r}} \overset{H_0}{\thicksim} F
(
\text{df}_\text{ESS}, \text{df}_\text{RSS}
)
$$
 

### Распределение Фишера

Значение F-статистики подчиняется F-распределению (распределению Фишера). Его форма зависит от двух значений степеней свободы --- $\text{df}_\text{ESS}$ (${df}_\text{e}$) и $\text{df}_\text{RSS}$ ($\text{df}_\text{r}$).


:::{#fig-F-dist}
```{r}
#| layout: [[50, 50]]


ggplot() +
  geom_function(fun = df, args = list(df1 = 1, df2 = 30), 
                n = 500, aes(color = "1")) +
  geom_function(fun = df, args = list(df1 = 2, df2 = 30), 
                n = 500, aes(color = "2")) +
  geom_function(fun = df, args = list(df1 = 3, df2 = 30), 
                n = 500, aes(color = "3")) +
  geom_function(fun = df, args = list(df1 = 4, df2 = 30), 
                n = 500, aes(color = "4")) +
  geom_function(fun = df, args = list(df1 = 5, df2 = 30), 
                n = 500, aes(color = "5")) +
  geom_function(fun = df, args = list(df1 = 6, df2 = 30), 
                n = 500, aes(color = "6")) +
  xlim(0, 5) +
  ylim(0, 1) +
  scale_color_discrete(labels = c(
    "1" = TeX("$df_e = 1, df_r = 30$"),
    "2" = TeX("$df_e = 2, df_r = 30$"),
    "3" = TeX("$df_e = 3, df_r = 30$"),
    "4" = TeX("$df_e = 4, df_r = 30$"),
    "5" = TeX("$df_e = 5, df_r = 30$"),
    "6" = TeX("$df_e = 6, df_r = 30$")
  )) +
  labs(x = "F", y = "Density", color = NULL)

ggplot() +
  geom_function(fun = df, args = list(df1 = 4, df2 = 10), 
                n = 500, aes(color = "1")) +
  geom_function(fun = df, args = list(df1 = 4, df2 = 30), 
                n = 500, aes(color = "2")) +
  geom_function(fun = df, args = list(df1 = 4, df2 = 50), 
                n = 500, aes(color = "3")) +
  geom_function(fun = df, args = list(df1 = 4, df2 = 100), 
                n = 500, aes(color = "4")) +
  geom_function(fun = df, args = list(df1 = 4, df2 = 150), 
                n = 500, aes(color = "5")) +
  geom_function(fun = df, args = list(df1 = 4, df2 = 200), 
                n = 500, aes(color = "6")) +
  xlim(0, 5) +
  ylim(0, 1) +
  scale_color_discrete(labels = c(
    "1" = TeX("$df_e = 4, df_r = 10$"),
    "2" = TeX("$df_e = 4, df_r = 30$"),
    "3" = TeX("$df_e = 4, df_r = 50$"),
    "4" = TeX("$df_e = 4, df_r = 100$"),
    "5" = TeX("$df_e = 4, df_r = 150$"),
    "6" = TeX("$df_e = 4, df_r = 200$")
  )) +
  labs(x = "F", y = "Density", color = NULL)
```
Распределение Фишера при различных значениях его параметров
:::

Как можно заметить, форма достаточно сильно меняется при увеличении первого числа степеней свободы, и не очень сильно, но всё же меняется, с увеличением второго числа степеней свободы.

Не трудно предположить, что для значения F-статистики будет вычисляться p-value, на основании которого будет осуществляться статистический вывод по стандартному алгоритму:

- Если $\mathsf p < \alpha$, то мы получаем значение F-статистики, не характерное для случая, когда нулевая гипотеза верна, что даёт нам основания отклонить нулевую гипотезу об отсутствии различий между группами и принять альтернативную, о том, что хотя бы между двумя какими-либо группами есть различия.
- Если $\mathsf p > \alpha$, то мы получаем значение F-статистики, характерное для случая, когда нулевая гипотеза верна, что не даёт нам оснований отклонить нулевую гипотезу об отсутствии различий между группами.

### Представление результатов дисперсионного анализа {#anovaI-results}

Результаты дисперсионного анализа обычно представляются в виде таблицы:

|Источник изменчивости|SS|df|MS|F|p|
|:---|:---:|:---:|:---:|:---:|:---:|
Фактор (объяснённая)|$\text{ESS}$|$\text{df}_\text{e}$|$\text{MS}_\text{e}$|$F_\text{obs}$|$\mathsf p$|
Случайная (остаточная)|$\text{RSS}$|$\text{df}_\text{r}$|$\text{MS}_\text{r}$|||

Если используется внутритекстовое описание, то оно оформляется так:

> По результатам дисперсионного анализа [не]было обнаружено значимая связь фактора X с зависимой переменной ($F(\text{df}_\text{e}, \text{df}_\text{r}) = 2.72, p < .001$).


## Размер эффекта в дисперсионном анализе {#anovaI-etasq}

Итак, с ошибкой первого рода мы, как обычно, быстро и легко поборолись. Теперь боремся с ошибкой второго рода. Принципиально ничего не меняется --- нам нужно понять, как считается размер эффекта, и рассчитать требуемую выборку.

В случае дисперсионного анализа размер эффекта --- это доля объясненной фактором дисперсии от всей дисперсии данных. Обозначается эта метрика $\eta^2$:

$$
\eta^2 = \frac{\text{ESS}}{\text{TSS}}
$$

Значения для интерпретации размера эффекта стандартно предлагаются такие:

|Значение $\eta^2$|Размер эффекта|
|:---:|:---:|
|0.01|Малый (small)|
|0.06|Средний (medium)|
|0.14|Большой (large)|

Собственно, эти значения вбиваем в специально обученное ПО --- и кайфуем!

:::{.callout-tip}
### $\eta^2$ vs $f^2$

Иногда в ПО для анализа мощности исследования можно встретить другую метрику размера эффекта для дисперсионного анализа --- $f^2$. Она может быть рассчитана на основе $\eta^2$, если внутри программы не предусмотрен пересчёт:

$$
f^2 = \frac{\eta^2}{1 - \eta^2}
$$
:::


## Визуализация результатов дисперсионного анализа {#anovaI-vis}

Визуализация результатов в дисперсионном анализе, с одной стороны, достаточно проста. Нам нужно отобразить точечные и интервальные оценки средних в группах, то есть нарисовать errorbar. В качестве интервальной оценки обычно отображается 95% доверительный интервал.

График будет выглядеть примерно так:

![](img/infer/anova_errorbar-1.png)


Интерпретировать подобные картинки мы уже с вами умеем --- если забыли, [напоминаю](#ci-meancomp).

Другим вариантом визуализации для дисперсионного анализа, который очень любят за рубежом, являются боксплоты. Вот как-то так:

![](img/infer/anova_boxplot-1.png)


Я искренне не понимаю, почему они их там так любят, ведь они не отображают ровно ничего, связанного с дисперсионным анализом — вспомните основные элеметы боксплота. Но их любят, их рисуют — ну, ладно. Кто мы такие, чтобы это запрещать.

Конечно, этот график хорош тем, что можно сделать что-то такое — иногда это может быть полезно:

![](img/infer/anova_boxplot2-1.png)


Но можно изобразить комбо! Оно называется violin plot:

![](img/infer/violinplot-1.png)


С одной стороны, он нам отобразит распределения зависимой переменной по группам, с другой стороны, мы можем дополнительно отобразить средние с доверительными интервалами, как сделано на графике выше. Ну, просто невероятно!


## Условия применения дисперсионного анализа {#anovaI-cond}

Как вы уже могли догадаться, дисперсионный анализ относится к группе параметрических статистических методов, поэтому выдвигается ряд требований к данным, на которых проводится такой анализ.

- Количественная непрерывная зависимая переменная
- Независимые между собой выборки
    - А если зависимые, то надо это учесть в модели
- Нормальное распределение признака *в генеральных совокупностях*, из которых извлечены выборки
- Равенство (гомогенность) дисперсий изучаемого признака в генеральных совокупностях, из которых извлечены выборки
    - Проверяется с помощью теста Левина
- Симметричное распределение выборочных данных
- Независимые наблюдения в каждой из выборок


### Гомогенность дисперсий {#anovaI-homogen}

Гомогенность дисперсии --- важное допущение. Поясним за него.

Как уже отмечено выше, модель дисперсионного анализа предполагает, что в генеральных совокупностях, из которых мы извлекли наши выборки, дисперсии изучаемого признака равны. Однако в силу неопределенности и вариативности это может быть и не так, поэтому нужно протестировать гипотезу о равенстве дисперсий в группах. Алгоритм статистического вывода нам известен, а в детали этого попутного теста мы, пожалуй, погружаться не будем.

Существует нескольок способов сравнить дисперсии в группах, наиболее популярными из которых являются тесты Левена (Leven's test) и Флигнера-Килина (Fligner–Killeen test).

Соответственно, мы хотим, чтобы его результаты не были статистически значимы, так как в этом случае мы не можем отклонить гипотезу о том, что дисперсии гомогенны. Если же результаты теста статистически значимы, то использование дисперсионного анализа может привести к искаженным результатам.


## Виды эффектов в дисперсионном анализе {#anovaI-effect}

Тот вид дисперсионного анализа, который мы рассмотрели --- это такой стандартный базовый дисперсионный анализ. Однако давайте пристально посмотрим на один момент:

> «пусть у нас три группы испытуемых — каждая группа проходит одно экспериментальное условие».

То есть каждой из экспериментальных групп у нас уникальные испытуемые. Такой эффект фактора в дисперсионном анализе называется **between-subject effect**, а в терминах эксперименталки это будут независимые выборки.

Но в жизни, как мы знаем, такой экспериментальный дизайн вряд ли адекватен. Скорее, мы бы сделали так, что один испытуемый проходит все три условия --- и нейтральное, и негативное, и позитивное. Да ещё и в случайном порядке. У нас получится, что от каждого испытуемого мы получаем три измерения --- с нейтрального, негативного и позитивного условия. Такой эффект фактора в дисперсионном анализе называется **within-subject effect**, сама ANOVA будет **дисперсионным анализом с повторными измерениями**, а выборки наблюдений буду зависимыми.
