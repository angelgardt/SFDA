# Корреляционный анализ {#infer-correlation}

```{r opts, echo=FALSE, eval=TRUE}
knitr::opts_chunk$set(echo = FALSE, eval = TRUE, warning = FALSE, message = FALSE, error = FALSE)
```

```{r}
library(tidyverse)
theme_set(theme_bw())
theme_update(legend.position = "bottom")
library(latex2exp)
```

Итак, мы переходим к рассмотрению конкретных статистических методов. Начнём, пожалуй, с наиболее интуитивного --- тестируем гипотезу о связи между двумя количественными переменными.


## Ковариация {#correlation-cov}

Мы хотим описать имеющиеся взаимосвязи как можно проще и опираясь на то, что у нас уже есть. Мы знаем, что дисперсия, или вариация (variance), заключает в себе информацию об изменчивости признака. Если мы хотим исследовать взаимосвязь между признаками, то логично будет посмотреть, как изменяется один из признаков при изменении другого --- иначе говоря, рассчитать совместную изменчивость признаков, или **ко-вариацию (co-variance)**.

Как мы её будем считать? Подумаем графически. Расположим две переменные на осях и сопоставим каждому имеющемуся наблюдению точку на плоскости.

:::{#fig-cor-scheme-pos-start}
![](img/infer/cor-scheme-pos-start.jpg)

Общий вид прямой взаимосвязи между двумя переменными
:::

Можно заметить, что точки на графике, отображающие наши наблюдения, как бы идут более-менее в направлении какой-то прямой --- серой, как на картинке:

:::{#fig-cor-scheme-pos-trend}

![](img/infer/cor-scheme-pos-trend.jpg)

Линия тренда, описывающая наблюдаемую закономерность
:::


Отметим средние значения по обеим переменным.

:::{#fig-cor-scheme-pos-means}
![](img/infer/cor-scheme-pos-means.jpg)

Положение наблюдений относительно средних по обеим переменным
:::


Разумеется, у нас наблюдаются отклонения наблюдений от среднего. Заметим, что отклонения могут быть *сонаправленными* --- одновременно по обеим переменным $X$ и $Y$ в положительную или отрицательную сторону (синие стрелки) --- или *разнонаправленными* --- в положительную сторону по одной из переменных и в отрицательную по другой, и наоборот (рыжие стрелки).


:::{#fig-cor-scheme-pos-signs}
![](img/infer/cor-scheme-pos-deviations.jpg)

Направление отклонений наблюдений от средних
:::

При этом произведения сонаправленных отклонений будет положительны, а разнонаправленных --- отрицательны.

:::{#fig-cor-scheme-pos-signs}
![](img/infer/cor-scheme-pos-signs.jpg)

Знаки произведений отклонений
:::

Получается, мы можем на основании согласованности отклонений уже сделать заключение о направлении связи, ведь произведение отклонений по обеим величинам будет положительно, если отклонения сонаправленны, и отрицательно, если они разнонаправленны. Остается только понять, как совместные отклонения организованы «в среднем» --- это и будет **ковариацией** двух величин:

$$
\text{cov}(X,Y) = \frac{1}{n-1} \sum_{i=1}^n (\overline x - x_i)(\overline y - y_i)
$$

Если у нас много сонаправленных отклонение, то значение ковариации будет положительно.

Аналогичное рассужение мы можем провести, если наблюдается обратное направление взаимосвязи.

Расположим две переменные на осях и сопоставим каждому имеющемуся наблюдению точку на плоскости.

:::{#fig-cor-scheme-neg-start}
![](img/infer/cor-scheme-neg-start.jpg)

Общий вид обратной взаимосвязи между двумя переменными
:::

Можно заметить, что точки на графике, отображающие наши наблюдения, так же идут более-менее в направлении какой-то прямой --- серой, как на картинке:

:::{#fig-cor-scheme-neg-trend}

![](img/infer/cor-scheme-neg-trend.jpg)

Линия тренда, описывающая наблюдаемую закономерность
:::


Отметим средние значения по обеим переменным.

:::{#fig-cor-scheme-neg-means}
![](img/infer/cor-scheme-neg-means.jpg)

Положение наблюдений относительно средних по обеим переменным
:::


Теперь у нас будет больше *разнонаправленных* отклонений, чем *сонаправленных*.

:::{#fig-cor-scheme-pos-signs}
![](img/infer/cor-scheme-pos-deviations.jpg)

Направление отклонений наблюдений от средних
:::

Соответственно, отрицательных произведений отклонений будет больше, чем положительных.

:::{#fig-cor-scheme-pos-signs}
![](img/infer/cor-scheme-pos-signs.jpg)

Знаки произведений отклонений
:::

Получается, в этом случае значение ковариации будет отрицательно.


> Кстати, а что такое ковариация переменной с самой собой?

:::{.callout-note appearance="minimal" collapse="true"}
### Вот что

$$
\text{cov}(X, X) = \frac{1}{n-1} \sum_{i=1}^n (\overline x - x_i)(\overline x - x_i) = \frac{1}{n-1} \sum_{i=1}^n (\overline x - x_i)^2 = \text{var}(X)
$$

:::

Если же взаимосвязи между переменными не наблюдается, то ковариация будет примерно равна нулю.

:::{#fig-cor-scheme-zero-signs layout-ncol=2}

![Общий вид отсутствия взаимосвязи между переменными](img/infer/cor-scheme-zero-start.jpg){#fig-cor-scheme-zero-start}

![Направления отклонения и знаки проиведения отклонений](img/infer/cor-scheme-zero-signs.jpg){#fig-cor-scheme-zero-signs}

Ситуация отсутствия взаимосвязи между переменными
:::

Однако здесь важно отметить, что ковариация улавливает только **линейную составляющую взаимосвязи** между признаками --- мы говорили выше, что точки идут как бы по некоторой прямой — поэтому если $\text{cov} (X,Y) = 0$, то мы можем сказать, что **между переменными нет линейной взаимосвязи**, однако это не значит, что между этими переменными нет никакой другой зависимости.

Если же **ковариация отлична от нуля**, то

- если её значение **положительно**, то мы можем заподозрить **прямую связь между переменными** (@fig-cor-scheme-pos-start),
- если же её значение **отрицательно**, то мы можем предположить **обратную связь между переменными** (@fig-cor-scheme-neg-start).

У ковариации есть **два важных недостатка**:

- это размерная величина, поэтому её значение **зависит от единиц измерения признаков**,
- она зависит от дисперсий признаков, поэтому **по её значению можно определить только направление связи** (прямая или обратная), однако ничего нельзя сказать о силе связи.

Поэтому нам нужно как-то модицифировать эту статистику, чтобы мы могли больше вытащить из её значения.


## Корреляция {#correlation-cor}

Раз ковариация зависит от дисперсии, то можно сделать некоторые математические преобразования, чтобы привести эмпирические распределения к какому-то одному виду --- сделать так, чтобы они имели одинаковое среднее и одинаковую дисперсию. С этой задачей прекрасно справляется [стандартизация](#normdist-stand). Напоминаю формулу:

$$
z_i = \frac{x_i - \overline x}{s_X}
$$

После такого преобразования среднее нашего распределения будет равно нулю, а стандартное отклонение --- единице. Это избавит нас от влияния дисперсии на значение ковариации. Ковариация двух стандартизированных случайных величин называется **корреляцией (correlation)**.

$$
\text{cov}(X^*, Y^*) = \frac{1}{n-1} X^* Y^* = \text{cor}(X, Y),
$$

где $X^*$ и $Y^*$ --- стандартизированные величины $X$ и $Y$ соответственно.

Корреляцию можно выразить через ковариацию:

$$
\begin{aligned}
\text{cor}(X, Y) &= \text{cov}(X^*, Y^*) = \\
&= \frac{1}{n-1} \sum_{i=1}^n \left( \frac{x_i - \overline x}{s_X} \right) \left( \frac{y_i - \overline y}{s_Y} \right) = \\
&= \frac{1}{s_X s_Y} \left( \frac{1}{n-1} \sum_{i=1}^n (x_i - \overline x) (y_i - \overline y) \right) = \frac{\text{cov}(X,Y)}{s_X s_Y}
\end{aligned}
$$
 

Если внимательно всмотреться в формулу, то можно обнаружить, что корреляция --- это не что иное, как *стандартизированное значение ковариации*.

Коэффициент корреляции имеет четкие пределы изменения: $[-1,1]$. Крайнее левое значение говорит о том, что присутствует *полная обратная линейная взаимосвязь*, крайнее правое --- что присутствует *полная прямая линейная взаимосвязь*. Как и ковариация, корреляция ловит только линейную составляющую связи, поэтому *нулевое* значение корреляции показывает, что между переменными *отсутствует линейная взаимосвязь*. Это всё еще не значит, что связи нет вовсе.

Преимущество корреляции над ковариацией в том, что она отражает не только направление,  нои силу связи:

:::{#tbl-cor-interp}
|Значение коэффициента|Интерпретация|
|:---:|:---:|
|$-1.0$ -- $-0.9$|очень сильная обратная связь|
$-0.9$ -- $-0.7$|сильная обратная связь|
$-0.7$ -- $-0.5$|средняя обратная связь|
$-0.5$ -- $-0.3$|слабая обратная связь|
$-0.3$ -- $0.0$|очень слабая обратная связь|
$0.0$ -- $0.3$|очень слабая прямая связь|
$0.3$ -- $0.5$|слабая прямая связь|
$0.5$ -- $0.7$|средняя прямая связь|
$0.7$ -- $0.9$|сильная прямая связь|
$0.9$ -- $1.0$|очень сильная прямая связь|

Стандартная интерпретация значений коэффициента корреляции
:::

Пронаблюдать можно [тут](https://rpsychologist.com/correlation/).


## Корреляция Пирсона {#correlation-pearson}

То, что мы только что обсудили, называется **коэффициентом корреляции Пирсона (Pearson's correlation)**. Этот коэффициент применим, когда мы работаем с двумя нормально распределенными величинами или хотя бы двумя величинами, измеренными в метрических шкалах. Еще раз напомним его формулу:

$$
r_{X_1, X_2} = \frac{\sum_{i=1}^n (\overline x_1 - x_{1i}) (\overline x_2 - x_{2i})}{\sqrt{\sum_{i=1}^n (\overline x_1 - x_{1i})^2 \cdot \sum_{i=1}^n  (\overline x_1 - x_{1i})^2}}
$$

Теперь задумаемся о том, можем ли мы сказать по значению коэффициента корреляции, отличается ли он от нуля? Вспомнив, что мы занимаемся статистикой, сразу ответим --- нет, ведь мы рассчитываем коэффициент корреляции на данных, а они содержат всякую вариативность и неопределенность. Нам придется предпринять дополнительные усилия, чтобы заявить, что обнаруженная нами связь --- это *не случайная* находка.


### Тестирование статистической значимости коэффициента корреляции {#correlation-stattest}

Итак, мы, как обычно, хотим ответить на вопрос, есть ли в генеральной совокупности связь между двумя изучаемыми переменными. Если в генеральной совокупности связь между признаками отсутствует, то есть $\rho_{X_1,X_2} = 0$, будет ли равен нулю $r_{X_1, X_2}$? Можно с уверенностью сказать, что не будет, так как выборочный коэффициент корреляции является случайной величиной. А мы помним, что вероятность принятия случайной величиной своего конкретного значения равна нулю.

Тогда необходимо протестировать статистическую гипотезу:

$$
\begin{aligned}
H_0&: \rho_{X_1, X_2} = 0 \\
H_1&: \rho_{X_1, X_2} \neq 0
\end{aligned}
$${#eq-rho-twosided}

Нулевая гипотеза гласит, что между признаками нет линейной взаимосвязи, альтернативная говорит --- что есть. Обычно мы не закладываем в гипотезу направление связи, потому что чаще всего и положительная, и отрицательная нас устроит --- это мы будем потом уже содержательно интерпретировать.

Однако в принципе нам никто не запрещает задать направленную альтернативную гипотезу:

$$
\begin{aligned}
H_0&: \rho_{X_1, X_2} \geq 0 \\
H_1&: \rho_{X_1, X_2} < 0
\end{aligned}
\quad \quad
\begin{aligned}
H_0&: \rho_{X_1, X_2} \leq 0 \\
H_1&: \rho_{X_1, X_2} > 0
\end{aligned}
$${#eq-rho-onesided}

Далее мы посмотрим, как это повлияет на ход тестирования гипотезы.

Гипотезу сформулировали, теперь надо подобрать статистический критерий. Он вот:

$$
t = \frac{r_{X_1,X_2} - \rho}{\sqrt{\dfrac{1 - r^2_{X_1,X_2}}{n-2}}}
\overset{\rho = 0}{=} \frac{r_{X_1,X_2}}{\sqrt{\dfrac{1 - r^2_{X_1,X_2}}{n-2}}}
\overset{H_0}{\thicksim} t(n-2)
$$ 

Это t-критерий, и значение его статистики подчиняется t-распределению, или распределению Стьюдента. Число степеней свободы определяется по формуле в скобках и равно $(n-2)$, где $n$ --- число наблюдений в выборке. Внешний вид t-распределения представлен на рисунке [-@fig-t-dist]

:::{#fig-t-dist}
```{r}
ggplot() +
  geom_function(aes(color = "1"), fun = dt, args = list(df = 2), n = 500) +
  geom_function(aes(color = "2"), fun = dt, args = list(df = 3), n = 500) +
  geom_function(aes(color = "3"), fun = dt, args = list(df = 4), n = 500) +
  geom_function(aes(color = "4"), fun = dt, args = list(df = 5), n = 500) +
  geom_function(aes(color = "5"), fun = dt, args = list(df = 10), n = 500) +
  geom_function(aes(color = "6"), fun = dt, args = list(df = 20), n = 500) +
  geom_function(aes(color = "7"), fun = dt, args = list(df = 30), n = 500) +
  xlim(-4, 4) +
  scale_color_discrete(
    labels = c(
      "1" = "df = 2",
      "2" = "df = 3",
      "3" = "df = 4",
      "4" = "df = 5",
      "5" = "df = 10",
      "6" = "df = 20",
      "7" = "df = 30"
      )) +
  labs(x = "t", y = "Плотность", color = "Степени свободы")
```

Общий вид t-распределения
:::


Может показаться, что оно похоже на нормальное распределение --- и это правда, но только с 30+ степеней свободы. А до этого можно наблюдать высокие хвосты.

Собственно, далее согласно уже знакомому нам алгоритму тестирования статистических гипотез мы рассчитываем статистику критерия, p-value для неё и делаем статистический вывод. Все так же, как и в случае с z-тестом.

Для двусторонней альтернативной гипотезы ([-@eq-rho-twosided]) критическая область будет задана с двух сторон (@fig-ttest-twosided). Для односторонней ([-@eq-rho-onesided]) --- соответственно, с одной стороны (@fig-ttest-onesided): для $\rho_{X_1,X_2}>0$ --- справа, для $\rho_{X_1,X_2}<0$ --- слева.

:::{#fig-ttest-twosided}
```{r}
ggplot() +
  stat_function(fun = dt, args = list(df = 20), n = 500) +
  stat_function(fun = dt, args = list(df = 20),
                geom = "area", xlim = c(-4, qt(0.05/2, df = 20)),
                fill = "red", alpha = .5) +
  stat_function(fun = dt, args = list(df = 20),
                geom = "area", xlim = c(qt(1-0.05/2, df = 20), 4),
                fill = "red", alpha = .5) +
  geom_vline(xintercept = c(qt(0.05/2, df = 20), qt(1 - 0.05/2, df = 20)),
             linetype = "dotted") +
  annotate(geom = "label", label = TeX("$H_1 : \\rho \\neq 0$"),
           x = 0, y = .1, size = 5) +
  xlim(-4, 4) +
  labs(x = "t", y = "Плотность")
```

Критическая область при двусторонней альтернативной гипотезе
:::


:::{#fig-ttest-onesided}
```{r}
#| layout: [[50, 50]]

ggplot() +
  stat_function(fun = dt, args = list(df = 20), n = 500) +
  stat_function(fun = dt, args = list(df = 20),
                geom = "area", xlim = c(qt(1-0.05, df = 20), 4),
                fill = "red", alpha = .5) +
  geom_vline(xintercept = qt(1 - 0.05, df = 20),
             linetype = "dotted") +
  annotate(geom = "label", label = TeX("$H_1 : \\rho > 0$"),
           x = 0, y = .1, size = 5) +
  xlim(-4, 4) +
  labs(x = "t", y = "Плотность")

ggplot() +
  stat_function(fun = dt, args = list(df = 20), n = 500) +
  stat_function(fun = dt, args = list(df = 20),
                geom = "area", xlim = c(-4, qt(0.05, df = 20)),
                fill = "red", alpha = .5) +
  geom_vline(xintercept = qt(0.05, df = 20),
             linetype = "dotted") +
  annotate(geom = "label", label = TeX("$H_1 : \\rho < 0$"),
           x = 0, y = .1, size = 5) +
  xlim(-4, 4) +
  labs(x = "t", y = "Плотность")
```

Критическая область при односторонней альтернативной гипотезе
:::



### Доверительный интервал для коэффициента корреляции {#correlation-ci}

Помимо того, что мы обязаны оценить статистическую значимость коэффициента корреляции, обычно мы еще строим доверительный интервал для него. С построением интервальной оценки возникают некоторые сложности. Мы не можем использовать рассмотренное выше распределение Стьюдента, так как оно строится для случая, когда верна нулевая гипотеза об отсутствии связи. Если же мы строим интервальную оценку, нас интересует случай наличия связи.

:::{.callout-note appearance="minimal" collapse="true"}
#### Статистики долго думали и придумали

как с этой ситуацией обойтись. Для построения доверительного интервала используется **z-преобразование Фишера**:

$$
\bar z_{X_1,X_2} = z(r_{X_1,X_2}) = \frac{1}{2} \ln \left( 
\frac{ 1 + r_{X_1,X_2} }{ 1 - r_{X_1,X_2} } 
\right)
\thicksim \mathcal N(\bar z_{X_1,X_2}, \tfrac{1}{n-3}),
$$

где $n$ --- объём выборки, а $r_{X_1,X_2}$ --- выборочный коэффициент корреляции.

Интервальную оценку для величины $\bar z_{X_1,X_2}$ уже можно построить, используя нормальное распределение. Далее путём обратного преобразования получаются значения границ интервала $(r_{X_1,X_2}^{\min}, \; r_{X_1,X_2}^{\max})$.
:::

Впрочем, не очень важно, как именно будет вычисляться доверительный интервал, потому что всё равно мы доверим построить доверительный интервал машине. Нам важно понять, как интерпретировать получившийся результат.

- Если в доверительный интервал коэффициента корреляции **попадает** $0$, значит коэффициент статистически **равен нулю**, то есть между изучаемыми переменными взаимосвязи нет.
Если в доверительный интервал коэффициента корреляции **не попадает** $0$, значит коэффициент статистически **отличен от нуля**, то есть между изучаемыми переменными взаимосвязь есть.


### Размер эффекта для коэффициента корреляции {#correlation-effsize}

Еще одна статистика, которая нам необходима --- это размер эффекта. На этом моменте мы начинаем беседовать об ошибке второго рода. До этого мы говорили только об ошибке первого рода и научились её контролировать с помощью уровня значимости.

В целом, **размер эффекта** --- это численное выражение силы взаимосвязи между переменными в генеральной совокупности.

Здесь нам необходимо призадуматься, и осознать, что вообще-то корреляция сама по себе выражает силу взаимосвязи между переменными. И, да, это правда --- **размером эффекта для коэффициента корреляции является сам коэффициент корреляции**. Удобненько.

Так, к сожалению, будет не всегда, но вот с корреляцией нам повезло.

Рекомендации по интерпретация [абсолютного значения] коэффициента корреляции с точки зрения размера эффекта для социальных наук такие:

:::{#tbl-cor-effect-size}
|Значение коэффициента|Размер эффекта|
|:---:|:---:|
|$0.1$|Малый (small)|
|$0.3$|Средний (medium)|
|$0.7$|Большой (large)|

Интерпретация значений корреляции с точки зрения размера эффекта
:::


### Расчет объема выборки для корреляционного анализа

Размер эффекта нам требуется для расчета объема выборки, необходимой для корректного корреляционного анализа. В чем идея?

Мы говорили, что ошибку второго рода мы контролирует c помощью статистической мощности, которая равна $1-\beta$, где $\beta$ --- вероятность ошибки второго рода. Конвенционально достаточным уровнем статистической мощности считается $80\%$, то есть нам надо предпринять все усилия, чтобы достичь такого уровня статистической мощности.

А что мы можем предпринять? Да, в общем-то только собрать достаточное количество наблюдений. Значит, надо рассчитать, сколько нам надо наблюдений, чтобы зафиксировать эффект, если он есть.

Собственно, статистическая мощность, размер эффекта и объем выборки связаны вот как:

$$
\begin{aligned}
\uparrow \text{объем выборки} &\Rightarrow \uparrow \text{статистическая мощность} \\
\uparrow \text{размер эффекта} &\Rightarrow \downarrow \text{требуемый объем выборки}
\end{aligned}
$$

Итого, если у нас есть ожидаемый размер эффекта и требуемый уровень статистической мощности, то мы можем рассчитать необходимый объем выборки. 

Формул не будет --- мы рассчитываем объем выборки в специально обученном ПО.

- Уровень статистической мощности (power) можно выбрать и более 80% --- это будет даже лучше.
- Размер эффекта можем взять из предыдущих исследований, на основе которых планируем текущее --- это лучше --- либо из рекомендаций по интерпретации размеров эффекта.
   - Так, в психологии большие корреляции --- это вообще большая редкость, поэтому берем значения для малого или среднего размеров эффекта.



### Визуализация корреляции {#correlation-vis}

Как можно отображать взаимосвязи между переменными? Один пример графика мы уже видели в самом начале --- это была **диаграмма рассеяния (scatterplot)**. Это достаточно простой и понятный график: по осям идут две количественные переменные, точки отображают наблюдения. В итоге получается облако точек. Чем более они вытянуты в линию, тем больше значение корреляции, чем более облако точек «круглое», тем значение кореляции меньше. Дополнительно обычно на таком графике отображают ещё линию тренда, чтобы более наглядно визуализировать линейный компонент взаимосвязи переменных. Построение такой линии отдельная довольно интересная задача --- мы вскоре ею займемся и назовем это регрессионный анализ.

Ниже примеры диаграмм рассения. Это визуализация результатов валидизации психометрической методики: на первом графике корреляция между итоговыми баллами двух опросников, а на втором --- баллов по отдельным шкалам валидизируемого опросника и общим баллом другого опросника.

:::{#fig-scatterplots layout-ncol=2}

![](img/infer/cor-scatterplot.png)

![](img/infer/cor-scatterplots.png)

Примеры диаграмм рассеяния
:::

[Можно потренироваться](http://guessthecorrelation.com/) визуально определять корреляцию по скаттерплотам.

Бывает, что нас интересует связь не между двумя какими-либо отдельными переменными, а много попарных корреляций между несколькими переменными. Для этого есть визуализация, называемая **корреляционная матрица (corrplot)**. По «осям» идут переменные, а на пересечении цветом отображается корреляция между переменными. Обычно холодные оттенки используются для положительных значений, а теплые --- для отрицательных. Чем интенсивнее оттенок, тем больше значение корреляции. В данном случае на графике отображены корреляции между пунктами отдельной шкалы опросника.

:::{#fig-corrplot}

![](img/infer/cor-corrplot.png)

Примеры корреляционной матрицы
:::



## Коэффициенты корреляции для разных шкал {#correlation-corscales}

Выше мы подробно обсудили корреляцию Пирсона и отметили, что этот коэффициент применяется для метрических шкал да ещё и нормально распределенных величин. А что же делать, если в наших данных не так?

### Параметрические и непараметрические критерии {#correlation-param}

Все статистические методы делятся на два типа: параметрические и непараметрические.

- **Параметрические методы** работают непосредственно с параметрами распределения исследуемых переменных.
    - Так, для вычисления корреляции Пирсона мы используем среднее и стандартное отклонение.
    - Как следствие, одно из условий[^norm-param] применимости таких методов --- нормальное распределение изучаемых переменных.
        - Если это требование не выполнено, мы не можем быть достаточно уверены в том, что статистический метод дает надежные результаты --- как минимум, потому что сами среднее и стандартное отклонение в случае скошенных распределения не особо хорошо описывают данные.
- **Непараметрические методы** не используют в вычислениях собственных статистик параметры распределений.
    - Они работают либо с частотами, либо c рангами --- в зависимости от того, какие переменные мы анализируем.
    - Поэтому если требования нормальности распределения не выполнено, используют методы этой группы.

Практически каждый параметрический критерий имеет своего непараметрического собрата. Отметим, что сам статистический вывод в случае непараметрических критериев осуществляется точно так же, как и в случае параметрических.

[^norm-param]: Хотя в ряде случаев даже параметрические статистические методы бывают довольно устойчивы к отклонению распределений от нормального.


### Непараметрические коэффициенты корреляции {#correlation-nonparam}

Для разных шкал разработаны разные коэффициенты корреляции. Оценки коэффициентов будут рассчитываться по-разному, но логика тестирования статистических гипотез остаётся одинаковой.

|Переменная $X$|Переменная $Y$|Мера связи|
|:---:|:---:|:---:|
|Интервальная или отношений|Интервальная или отношений|Корреляция Пирсона|
|Ранговая, интервальная или отношений|Ранговая, интервальная или отношений|Корреляция Спирмена|
|Ранговая|Ранговая|Корреляция Кенделла|

У непараметрических критериев формулы несколько зубодробительны, поэтому оставим их покоиться с миром. В статистическом ПО нужно будет просто выбрать подходящий критерий из предложенных.


## Другие корреляции {#correlation-other}

> Этот раздел для отчаянных. Вас никто об этом ни на экзамене, ни, возможно, в жизни не спросит.

Можно коррелировать не только количественные и ранговые шкалы между собой, но и категориальные тоже:

|Переменная $X$|Переменная $Y$|Мера связи|
|:---:|:---:|:---:|
|Дихотомическая|Дихотомическая|$\varphi$-коэффициент|
|Дихотомическая|Ранговая|Рангово-бисериальный коэффициент|
|Дихотомическая|Интервальная или отношений|Бисериальный коэффициент|


### $\varphi$-коэффициент {#correlation-phi}

Этот коэффициент позволяет рассчитать корреляцию между двумя дихотомическими шкалами. Он основан на расчёте статистики $\chi^2$. По двум дихотомическим переменным строится, [как мы узнаем](#infer-categorical), таблица сопряженности и рассчитывается $\chi^2$. Далее он нормируется следующим образом, чтобы получить значения от 0 до 1, которые можно интерпретироват аналогично коэффициенту корреляции:

$$
\varphi = \sqrt{\frac{\chi^2}{N}},
$$

где $N$ --- общее количество наблюдений.


### Бисериальный коэффициент корреляции {#correlation-biser}

Этот коэффициент используется для вычисления корреляции между количественной ($Y$) и категориальной ($X$) шкалой и рассчитывается следующим образом:

$$
r = \frac{\overline y_{x_1} - \overline y_{x_2}}{s_Y} \sqrt{\frac{n_1 n_2}{N}},
$$

где $\overline y_{x_1}$ --- среднее по переменной $Y$ по элементам группы $x_1$,
$\overline y_{x_2}$ --- среднее по переменной $Y$ по элементам группы $x_2$,
$n_1$ --- число элементов в группе $x_1$,
$n_2$ --- число элементов в группе $x_2$,
$N$ --- общее число элементов.

Важно отметить, что несмотря на то, что значение коэффициента может быть как положительным, так и отрицательным, это не влияет на интерпретацию. Это одно из исключений из общего правила.


### Рангово-бисериальный коэффициент корреляции {#correlation-rankbiser}

Если у нас не количественная, а ранговая шкала, то применяется рангово-бисериальный коэффициент:

$$
r = \frac{2(\overline x_1 - \overline x_2)}{N},
$$

где $\overline x_1$ --- средний ранг в группе $x_1$, $\overline x_2$ --- средний ранг в группе $x_2$, $N$ --- общее количество наблюдений.
